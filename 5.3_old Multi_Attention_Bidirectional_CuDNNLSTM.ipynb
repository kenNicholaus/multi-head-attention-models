{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf you are running locally then \\n1. reboot your local machine\\n2. create an environment called 'colab' using anaconda prompt\\nif you have a gpu\\nconda create -n colab python tensorflow-gpu \\nif not \\nconda create -n colab python tensorflow\\n3. to install jupyter notebook\\nconda install jupyter notebook\\n4. to go to the 'colab' environment\\nactivate colab\\n5. change file path to locate this notebook and then type 'jupyter notebook'\\n\\nIf you use colab\\n1. save the data file in your google drive\\n2. goto colab and start running the code\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are running locally then \n",
    "1. reboot your local machine\n",
    "2. create an environment called 'colab' using anaconda prompt\n",
    "if you have a gpu\n",
    "conda create -n colab python tensorflow-gpu \n",
    "if not \n",
    "conda create -n colab python tensorflow\n",
    "3. to install jupyter notebook\n",
    "conda install jupyter notebook\n",
    "4. to go to the 'colab' environment\n",
    "activate colab\n",
    "5. change file path to locate this notebook and then type 'jupyter notebook'\n",
    "\n",
    "If you use colab\n",
    "1. save the data file in your google drive\n",
    "2. goto colab and start running the code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install prominent libraries with specific versions\n",
    "\n",
    "#!pip install tensorflow==1.15.0\n",
    "#!pip install keras==2.2.4-tf\n",
    "#!pip install pandas==0.25.1\n",
    "#!pip install sklearn==0.21.3\n",
    "#!pip install matplotlib==3.2.1\n",
    "#!pip install hyperas\n",
    "#!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "sJnnN6xG_yaM",
    "outputId": "b6420ce8-7f05-440c-e2ce-479b5bc1cafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, CuDNNLSTM, GRU, Input, Activation, Flatten, BatchNormalization, Reshape,Bidirectional\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, History, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras import regularizers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from tensorflow.python.keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from sklearn.preprocessing import *\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from tensorflow.python.keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from tensorflow.python.keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import initializers, regularizers, constraints\n",
    "from tensorflow.python.keras.initializers import *\n",
    "import hyperas\n",
    "import hyperopt\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.__version__ =  1.15.0\n",
      "sklearn.__version__ =  0.23.1\n",
      "numpy.__version__ =  1.19.1\n",
      "pandas.__version__ =  1.1.0\n",
      "matplotlib.__version__ =  3.2.2\n"
     ]
    }
   ],
   "source": [
    "#Get library versions\n",
    "print(\"tensorflow.__version__ = \", tf.__version__)\n",
    "# import tensorflow.python.keras\n",
    "# print(\"keras.__version__ = \", tensorflow.python.keras.__version__)\n",
    "import sklearn \n",
    "print(\"sklearn.__version__ = \", sklearn.__version__)\n",
    "print(\"numpy.__version__ = \", np.__version__)\n",
    "print(\"pandas.__version__ = \", pd.__version__)\n",
    "import matplotlib\n",
    "print(\"matplotlib.__version__ = \", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed to generate reproduceable results\n",
    "from numpy.random import seed\n",
    "seed(56)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(56)\n",
    "random.seed(56)\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they do not exist\n",
    "def build_path(dirName):\n",
    "    try:\n",
    "        os.makedirs(dirName)    \n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    except:\n",
    "        print(\"Directory \" , dirName ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12299999786606676982\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7125503181\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18438792381966152159\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# to check if GPU is getting used locally.....you need to see CPU as well as GPU in the output\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOs6JYN__51O"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "  i = 7 # the label target - number of days to predict from the input date\n",
    "  p = 7 #Number of days for target calculated in the data set\n",
    "  batch_size=512\n",
    "  CLASSES = 2\n",
    "  time_steps = 7\n",
    "  ticker='^GSPC'\n",
    "    \n",
    "  # read data\n",
    "  df=pd.read_csv('../^GSPC_7_days_0_return.csv', index_col = 0, parse_dates = True)\n",
    "    \n",
    "  #add additional rolling mean data  \n",
    "  rm_window =30\n",
    "  rolling_mean = []\n",
    "  for a in range(2,rm_window+1):\n",
    "    df[ticker+'rm_'+str(a)] = df[ticker].rolling(window=rm_window,center=False).mean()\n",
    "    rolling_mean.append(ticker+'rm_'+str(a))\n",
    "    \n",
    "  # create label\n",
    "  targets=pd.DataFrame([])\n",
    "  for j in range (1, p+1):\n",
    "    targets=targets.append(df[ticker+'_{}d_target'.format(j)])\n",
    "    targets=targets.append(df[ticker+'_{}d'.format(j)])\n",
    "  targets=targets.T\n",
    "  df=df.drop(targets.columns, axis=1)\n",
    "  df=df[rm_window:-i]\n",
    "  targets=targets[rm_window:-i]\n",
    "  y=targets['^GSPC_{}d_target'.format(i)]\n",
    "\n",
    "  #check for NaN and remove\n",
    "  df.isna().mean().sum()\n",
    "  y.isna().mean().sum()\n",
    "  remove_list=[]\n",
    "  for i in df.isnull().any().iteritems():\n",
    "    if i[1] == True:\n",
    "      remove_list.append(i[0])\n",
    "  df=df.drop(remove_list, axis=1)\n",
    "  df.isnull().any().mean()\n",
    " \n",
    "  # add percent change\n",
    "  df=df.pct_change()\n",
    "  df=df.replace([np.inf, -np.inf],np.nan) \n",
    "  df.fillna(0, inplace=True)\n",
    "  df.isnull().any().mean()\n",
    "    \n",
    "  # apply preprocessing \n",
    "  x_scaler=RobustScaler()\n",
    "  x = x_scaler.fit_transform(df)\n",
    "  # x_pred = x_scaler.fit_transform(x_pred)\n",
    "  del df\n",
    "  y=y.values\n",
    "    \n",
    "  # apply time steps\n",
    "  def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "      v = X[i:(i + time_steps)]\n",
    "      Xs.append(v)\n",
    "      ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "  x, y = create_dataset(x, y, time_steps)\n",
    "\n",
    "  # create train and test dataset\n",
    "  x_train, x_test, y_train, y_test=train_test_split(x,y, train_size=0.7, random_state=54)\n",
    "  x_train = x_train.astype('float32')\n",
    "  x_test = x_test.astype('float32')\n",
    "  #y_train = y_train.astype('float32')\n",
    "  #y_test = y_test.astype('float32')\n",
    "  y_train = np_utils.to_categorical(y_train, CLASSES, \n",
    "                                    #dtype='float32'\n",
    "                                   )\n",
    "  y_test = np_utils.to_categorical(y_test, CLASSES, \n",
    "                                   #dtype='float32'\n",
    "                                  )\n",
    "    \n",
    "  # adjustment for batch_size\n",
    "  train_start = x_train.shape[0]%batch_size\n",
    "  test_start = x_test.shape[0]%batch_size\n",
    "  x_train = x_train[train_start:]\n",
    "  y_train = y_train[train_start:]\n",
    "  x_test = x_test[test_start:]\n",
    "  y_test = y_test[test_start:]\n",
    "  embed_size = 60\n",
    "\n",
    "  return x_train, x_test, y_train, y_test, batch_size, time_steps, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc8hDOIbADgQ"
   },
   "outputs": [],
   "source": [
    "def create_multi_attention_bidirectional_cudnnlstm_model(x_train, x_test, y_train, y_test, batch_size, time_steps, embed_size):\n",
    "    \n",
    "    class LayerNormalization(Layer):\n",
    "        def __init__(self, eps=1e-6, **kwargs):\n",
    "            self.eps = eps\n",
    "            super(LayerNormalization, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],initializer=Ones(), trainable=True)\n",
    "            self.beta = self.add_weight(name='beta', shape=input_shape[-1:],initializer=Zeros(), trainable=True)\n",
    "            super(LayerNormalization, self).build(input_shape)\n",
    "        def call(self, x):\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.std(x, axis=-1, keepdims=True)\n",
    "            return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return input_shape\n",
    "\n",
    "    class ScaledDotProductAttention():\n",
    "        def __init__(self, d_model, attn_dropout=0.1):\n",
    "            self.temper = np.sqrt(d_model)\n",
    "            self.dropout = Dropout(attn_dropout)\n",
    "        def __call__(self, q, k, v, mask):\n",
    "            attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "            if mask is not None:\n",
    "                mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "                attn = Add()([attn, mmask])\n",
    "            attn = Activation('softmax')(attn)\n",
    "            attn = self.dropout(attn)\n",
    "            output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "            return output, attn\n",
    "\n",
    "    class MultiHeadAttention():\n",
    "        # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "        def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "            self.mode = mode\n",
    "            self.n_head = n_head\n",
    "            self.d_k = d_k\n",
    "            self.d_v = d_v\n",
    "            self.dropout = dropout\n",
    "            if mode == 0:\n",
    "                self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "                self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "                self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "            elif mode == 1:\n",
    "                self.qs_layers = []\n",
    "                self.ks_layers = []\n",
    "                self.vs_layers = []\n",
    "                for _ in range(n_head):\n",
    "                    self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                    self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                    self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "            self.attention = ScaledDotProductAttention(d_model)\n",
    "            self.layer_norm = LayerNormalization() if use_norm else None\n",
    "            self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "        def __call__(self, q, k, v, mask=None):\n",
    "            d_k, d_v = self.d_k, self.d_v\n",
    "            n_head = self.n_head\n",
    "\n",
    "            if self.mode == 0:\n",
    "                qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "                ks = self.ks_layer(k)\n",
    "                vs = self.vs_layer(v)\n",
    "\n",
    "                def reshape1(x):\n",
    "                    s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                    x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                    x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                    x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                    return x\n",
    "                qs = Lambda(reshape1)(qs)\n",
    "                ks = Lambda(reshape1)(ks)\n",
    "                vs = Lambda(reshape1)(vs)\n",
    "\n",
    "                if mask is not None:\n",
    "                    mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "                head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "\n",
    "                def reshape2(x):\n",
    "                    s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                    x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                    x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                    x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                    return x\n",
    "                head = Lambda(reshape2)(head)\n",
    "            elif self.mode == 1:\n",
    "                heads = []; attns = []\n",
    "                for i in range(n_head):\n",
    "                    qs = self.qs_layers[i](q)   \n",
    "                    ks = self.ks_layers[i](k) \n",
    "                    vs = self.vs_layers[i](v) \n",
    "                    head, attn = self.attention(qs, ks, vs, mask)\n",
    "                    heads.append(head); attns.append(attn)\n",
    "                head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "                attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "            outputs = self.w_o(head)\n",
    "            outputs = Dropout(self.dropout)(outputs)\n",
    "            if not self.layer_norm: return outputs, attn\n",
    "            # outputs = Add()([outputs, q]) # sl: fix\n",
    "            return self.layer_norm(outputs), attn\n",
    "\n",
    "    class PositionwiseFeedForward():\n",
    "        def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "            self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "            self.w_2 = Conv1D(d_hid, 1)\n",
    "            self.layer_norm = LayerNormalization()\n",
    "            self.dropout = Dropout(dropout)\n",
    "        def __call__(self, x):\n",
    "            output = self.w_1(x) \n",
    "            output = self.w_2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = Add()([output, x])\n",
    "            return self.layer_norm(output)\n",
    "\n",
    "    class EncoderLayer():\n",
    "        def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "            self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "            self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "        def __call__(self, enc_input, mask=None):\n",
    "            output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "            output = self.pos_ffn_layer(output)\n",
    "            return output, slf_attn\n",
    "\n",
    "\n",
    "    def GetPosEncodingMatrix(max_len, d_emb):\n",
    "        pos_enc = np.array([\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "            if pos != 0 else np.zeros(d_emb) \n",
    "                for pos in range(max_len)\n",
    "                ])\n",
    "        pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "        pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "        return pos_enc\n",
    "\n",
    "    def GetPadMask(q, k):\n",
    "        ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "        mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "        mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "        return mask\n",
    "\n",
    "    def GetSubMask(s):\n",
    "        len_s = tf.shape(s)[1]\n",
    "        bs = tf.shape(s)[:1]\n",
    "        mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "        return mask\n",
    "\n",
    "    class Transformer():\n",
    "        def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \\\n",
    "                  d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n",
    "                  share_word_emb=False, **kwargs):\n",
    "            self.name = 'Transformer'\n",
    "            self.len_limit = len_limit\n",
    "            self.src_loc_info = False # True # sl: fix later\n",
    "            self.d_model = d_model\n",
    "            self.decode_model = None\n",
    "            d_emb = d_model\n",
    "\n",
    "            pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n",
    "                                weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n",
    "\n",
    "            i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n",
    "\n",
    "            self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n",
    "                                   word_emb=i_word_emb, pos_emb=pos_emb)\n",
    "\n",
    "\n",
    "        def get_pos_seq(self, x):\n",
    "            mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "            pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "            return pos * mask\n",
    "\n",
    "        \n",
    "   \n",
    "    \n",
    "    x0 = Input(batch_shape=(batch_size, x_train.shape[1], x_train.shape[2]))\n",
    "    x1 = Bidirectional(CuDNNLSTM(units = {{choice([32, 64, 128])}}, name='x1',  kernel_regularizer=regularizers.l2(0.03), \n",
    "              bias_regularizer=regularizers.l1(0.03),\n",
    "              return_sequences=True, stateful = True))(x0)\n",
    "    x2 = BatchNormalization(name='x2')(x1)\n",
    "    x3 = Dropout({{uniform(0, 1)}},name='x3')(x2)\n",
    "    \n",
    "    \n",
    "    if {{choice(['three', 'four','five'])}} == 'three':\n",
    "        x4, slf_attn = MultiHeadAttention(n_head=3, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(x3, x3, x3)\n",
    "    elif 'four':\n",
    "        x4, slf_attn = MultiHeadAttention(n_head=4, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(x3, x3, x3)\n",
    "    elif 'five':\n",
    "        x4, slf_attn = MultiHeadAttention(n_head=5, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(x3, x3, x3)   \n",
    "    \n",
    "    x5 = Bidirectional(CuDNNLSTM(units = {{choice([32, 64, 128])}},name='x5',  kernel_regularizer=regularizers.l2(0.03), \n",
    "              bias_regularizer=regularizers.l1(0.03),\n",
    "              return_sequences=False,stateful = True))(x4)\n",
    "    x6 = BatchNormalization(name='x6')(x5)\n",
    "    x7 = Dropout({{uniform(0, 1)}},name='x7')(x6)\n",
    "    \n",
    "    if {{choice(['dense', 'nodense'])}} == 'dense':\n",
    "    \n",
    "        x8 = Dense(units = {{choice([8, 16, 32])}}, name='x8', kernel_regularizer=regularizers.l2(0.03), bias_regularizer=regularizers.l1(0.03),\n",
    "                   activation='relu')(x7)\n",
    "        x9 = Dropout({{uniform(0, 1)}}, name='x9')(x8)\n",
    "    \n",
    "        outp = Dense(2, activation='softmax')(x9)\n",
    "    else:\n",
    "        outp = Dense(2, activation='softmax')(x7)\n",
    "        \n",
    "\n",
    "    model = Model(inputs=[x0], outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer = Adam(lr=0.001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    result = model.fit(x_train,y_train,batch_size=batch_size,epochs=100,validation_data=(x_test, y_test), verbose=0)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "\n",
    "    print('Best validation acc of epoch:', result.history['val_acc'])\n",
    "    print('Train acc of epoch:', result.history['acc'])\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HcxtusN4AK8y",
    "outputId": "43d151c2-cf3b-4ab8-e95f-7c48cef1da49",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8704, 7, 2251) (8704, 2) (3584, 7, 2251) (3584, 2)\n",
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522265608>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522265608>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522265608>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522265608>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model\"                                                                                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_1 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional (Bidirectional)   (512, 7, 128)        1186304     input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense (Dense)                   (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_1 (Dense)                 (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda (Lambda)                 (2048, 7, 64)        0           dense[0][0]                                           \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_1 (Lambda)               (2048, 7, 64)        0           dense_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_3 (Lambda)               (2048, 7, 7)         0           lambda[0][0]                                          \n",
      "                                                                 lambda_1[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "activation (Activation)         (2048, 7, 7)         0           lambda_3[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_2 (Dense)                 (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout (Dropout)               (2048, 7, 7)         0           activation[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_2 (Lambda)               (2048, 7, 64)        0           dense_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_4 (Lambda)               (2048, 7, 64)        0           dropout[0][0]                                         \n",
      "                                                                 lambda_2[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_5 (Lambda)               (512, 7, 256)        0           lambda_4[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed (TimeDistribut (512, 7, 128)        32896       lambda_5[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_1 (Dropout)             (512, 7, 128)        0           time_distributed[0][0]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization (LayerNorma (512, 7, 128)        256         dropout_1[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_1 (Bidirectional) (512, 64)            41472       layer_normalization[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 64)            256         bidirectional_1[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 64)            0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_4 (Dense)                 (512, 2)             130         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,360,130                                                                                                \n",
      "Trainable params: 1,359,746                                                                                            \n",
      "Non-trainable params: 384                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.577846, 0.61997765, 0.6498326, 0.6635045, 0.66936386, 0.67410713, 0.69280136, 0.71428573, 0.703962, 0.6749442, 0.68498886, 0.71875, 0.6941964, 0.71372765, 0.71651787, 0.71651787, 0.70758927, 0.7419085, 0.7416295, 0.7433036, 0.73939735, 0.7282366, 0.73995537, 0.750558, 0.74609375, 0.74665177, 0.76311386, 0.74637276, 0.73939735, 0.7580915, 0.7480469, 0.7354911, 0.7589286, 0.7407924, 0.75279015, 0.7566964, 0.7486049, 0.75446427, 0.75, 0.7530692, 0.750279, 0.75362724, 0.750837, 0.74609375, 0.749163, 0.7438616, 0.74637276, 0.7564174, 0.7586495, 0.7575335, 0.75530136, 0.7488839, 0.75558037, 0.7589286, 0.7564174, 0.764788, 0.7597656, 0.76925224, 0.76032364, 0.7561384, 0.7572545, 0.75223213, 0.7594866, 0.76004463, 0.76953125, 0.77008927, 0.73800224, 0.7511161, 0.74581474, 0.75530136, 0.7575335, 0.7594866, 0.765067, 0.7589286, 0.74637276, 0.75530136, 0.749442, 0.750837, 0.74665177, 0.75390625, 0.7642299, 0.7561384, 0.7594866, 0.75362724, 0.7566964, 0.77120537, 0.75362724, 0.7488839, 0.7625558, 0.76199776, 0.7675781, 0.7642299, 0.76143974, 0.76004463, 0.76227677, 0.74720985, 0.7578125, 0.7645089, 0.7575335, 0.7586495]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.549977, 0.61546415, 0.6616498, 0.70369947, 0.73138785, 0.73862594, 0.7567785, 0.76482075, 0.77550554, 0.7725184, 0.7810202, 0.78527117, 0.78527117, 0.7899816, 0.79308367, 0.7959559, 0.7991728, 0.8048024, 0.799977, 0.7994026, 0.8074449, 0.8075597, 0.8000919, 0.80296415, 0.8043428, 0.8118107, 0.8080193, 0.8044577, 0.8074449, 0.8153722, 0.8191636, 0.8102022, 0.8126149, 0.816636, 0.81606156, 0.8157169, 0.8099724, 0.8125, 0.8080193, 0.8181296, 0.80859375, 0.813534, 0.81548715, 0.8105469, 0.8096278, 0.8066406, 0.8227252, 0.81548715, 0.8153722, 0.81893384, 0.8201976, 0.8150276, 0.8143382, 0.8153722, 0.8205423, 0.8247932, 0.8201976, 0.82973343, 0.8274357, 0.81824446, 0.8206572, 0.8172105, 0.82077205, 0.8249081, 0.82295495, 0.8290441, 0.82674634, 0.8174403, 0.81767005, 0.8153722, 0.8231847, 0.8199678, 0.8152574, 0.8161765, 0.8213465, 0.82169116, 0.8196232, 0.82548255, 0.82077205, 0.8228401, 0.82295495, 0.8284697, 0.83019304, 0.8181296, 0.82605696, 0.8224954, 0.82295495, 0.8190487, 0.8235294, 0.8196232, 0.82605696, 0.81893384, 0.8204274, 0.82387406, 0.8224954, 0.8214614, 0.81985295, 0.8299632, 0.8214614, 0.8293888]\n",
      "  5%|██▍                                             | 1/20 [01:29<28:21, 89.57s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Large dropout rate: 0.964957 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522E0A3C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522E0A3C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522E0A3C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024522E0A3C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_1\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_2 (Bidirectional) (512, 7, 256)        2438144     input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_2[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_5 (Dense)                 (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_6 (Dense)                 (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_6 (Lambda)               (2048, 7, 64)        0           dense_5[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_7 (Lambda)               (2048, 7, 64)        0           dense_6[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_9 (Lambda)               (2048, 7, 7)         0           lambda_6[0][0]                                        \n",
      "                                                                 lambda_7[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_1 (Activation)       (2048, 7, 7)         0           lambda_9[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_7 (Dense)                 (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_2 (Dropout)             (2048, 7, 7)         0           activation_1[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_8 (Lambda)               (2048, 7, 64)        0           dense_7[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_10 (Lambda)              (2048, 7, 64)        0           dropout_2[0][0]                                       \n",
      "                                                                 lambda_8[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_11 (Lambda)              (512, 7, 256)        0           lambda_10[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_1 (TimeDistrib (512, 7, 64)         16448       lambda_11[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_3 (Dropout)             (512, 7, 64)         0           time_distributed_1[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_1 (LayerNor (512, 7, 64)         128         dropout_3[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_3 (Bidirectional) (512, 128)           66560       layer_normalization_1[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_3[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_9 (Dense)                 (512, 2)             258         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,719,682                                                                                                \n",
      "Trainable params: 2,718,914                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5499442, 0.5552455, 0.55050224, 0.54185265, 0.55691963, 0.5594308, 0.5555245, 0.5546875, 0.55998886, 0.5555245, 0.5563616, 0.55217636, 0.55245537, 0.5541295, 0.5546875, 0.5532924, 0.5560826, 0.5552455, 0.5544085, 0.55998886, 0.55859375, 0.55998886, 0.5577567, 0.55189735, 0.5558036, 0.55831474, 0.54938614, 0.561663, 0.55887276, 0.562221, 0.5541295, 0.55719864, 0.57477677, 0.56529015, 0.5890067, 0.58342636, 0.57561386, 0.58231026, 0.5722656, 0.5867745, 0.5867745, 0.58844864, 0.5867745, 0.6015625, 0.5859375, 0.59905136, 0.5890067, 0.5856585, 0.59654015, 0.6007255, 0.59933037, 0.60379463, 0.61216515, 0.5923549, 0.60379463, 0.6007255, 0.609375, 0.608538, 0.6157924, 0.60714287, 0.6015625, 0.5867745, 0.5906808, 0.5867745, 0.6004464, 0.6029576, 0.594308, 0.6032366, 0.6032366, 0.6018415, 0.6035156, 0.609933, 0.6110491, 0.6238839, 0.61467636, 0.6107701, 0.61383927, 0.60574776, 0.60658485, 0.60435265, 0.62248886, 0.6063058, 0.5998884, 0.6074219, 0.6152344, 0.61495537, 0.61467636, 0.609654, 0.6202567, 0.6180245, 0.62053573, 0.6107701, 0.61328125, 0.6155134, 0.62248886, 0.61356026, 0.61383927, 0.60714287, 0.6141183, 0.625279]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5080423, 0.515625, 0.5289522, 0.5365349, 0.532284, 0.5394072, 0.5344669, 0.5265395, 0.5359605, 0.53825825, 0.53825825, 0.5391774, 0.5467601, 0.542739, 0.54331344, 0.5421645, 0.5453814, 0.5415901, 0.5502068, 0.5525046, 0.5440028, 0.5466452, 0.5523897, 0.5399816, 0.55985755, 0.5592831, 0.55307907, 0.56077665, 0.5639936, 0.5599724, 0.5603171, 0.5637638, 0.56675094, 0.5669807, 0.56870407, 0.5724954, 0.5714614, 0.56824446, 0.5670956, 0.5704274, 0.5788143, 0.57433367, 0.57410383, 0.5736443, 0.5721507, 0.57456344, 0.5746783, 0.5800781, 0.57858455, 0.5840993, 0.58111215, 0.5777803, 0.58203125, 0.5830653, 0.5846737, 0.5808824, 0.58547795, 0.581227, 0.58513325, 0.5813419, 0.58455884, 0.582261, 0.58674175, 0.5905331, 0.5847886, 0.58076745, 0.5940947, 0.5828355, 0.58800554, 0.5872013, 0.5849035, 0.5890395, 0.5857077, 0.58972883, 0.58536303, 0.5955882, 0.5883502, 0.5938649, 0.5855928, 0.58547795, 0.592716, 0.59076285, 0.58432907, 0.5883502, 0.5881204, 0.5904182, 0.58995867, 0.5905331, 0.5831801, 0.59317553, 0.59110755, 0.58949906, 0.5982307, 0.5886949, 0.5884651, 0.59317553, 0.58731616, 0.5846737, 0.59329045, 0.59018844]\n",
      " 10%|████▊                                           | 2/20 [03:08<27:45, 92.52s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Large dropout rate: 0.864886 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.765361 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C2DA2388>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C2DA2388>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C2DA2388>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C2DA2388>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      " 10%|████▊                                           | 2/20 [03:09<27:45, 92.52s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Large dropout rate: 0.980758 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_3 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_4 (Bidirectional) (512, 7, 128)        1186304     input_3[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_4[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_10 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_11 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_12 (Lambda)              (2048, 7, 64)        0           dense_10[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_13 (Lambda)              (2048, 7, 64)        0           dense_11[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_15 (Lambda)              (2048, 7, 7)         0           lambda_12[0][0]                                       \n",
      "                                                                 lambda_13[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_2 (Activation)       (2048, 7, 7)         0           lambda_15[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_12 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_4 (Dropout)             (2048, 7, 7)         0           activation_2[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_14 (Lambda)              (2048, 7, 64)        0           dense_12[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_16 (Lambda)              (2048, 7, 64)        0           dropout_4[0][0]                                       \n",
      "                                                                 lambda_14[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_17 (Lambda)              (512, 7, 256)        0           lambda_16[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_2 (TimeDistrib (512, 7, 128)        32896       lambda_17[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_5 (Dropout)             (512, 7, 128)        0           time_distributed_2[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_2 (LayerNor (512, 7, 128)        256         dropout_5[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_5 (Bidirectional) (512, 128)           99328       layer_normalization_2[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_5[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            4128        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_14 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,422,306                                                                                                \n",
      "Trainable params: 1,421,794                                                                                            \n",
      "Non-trainable params: 512                                                                                              \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation acc of epoch:                                                                                          \n",
      "[0.46763393, 0.5139509, 0.530692, 0.55022323, 0.55050224, 0.5398995, 0.5485491, 0.53376114, 0.5404576, 0.5421317, 0.54938614, 0.55106026, 0.54910713, 0.5452009, 0.5485491, 0.5541295, 0.5558036, 0.55691963, 0.5535714, 0.5541295, 0.5558036, 0.5555245, 0.5555245, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4622013, 0.4732307, 0.4908088, 0.5027574, 0.5167739, 0.5165441, 0.5155101, 0.512523, 0.5080423, 0.5034467, 0.512523, 0.5184972, 0.5153952, 0.5153952, 0.516659, 0.51160383, 0.5089614, 0.5163143, 0.5267693, 0.51619947, 0.5179228, 0.51700366, 0.51700366, 0.5252757, 0.5237822, 0.5227482, 0.53963697, 0.53044575, 0.53515625, 0.5440028, 0.5392923, 0.5472197, 0.5421645, 0.543773, 0.5443474, 0.5519301, 0.5521599, 0.5459559, 0.55066633, 0.55078125, 0.55824906, 0.56169575, 0.55549175, 0.5597426, 0.5568704, 0.5661765, 0.5672105, 0.561466, 0.56640625, 0.5681296, 0.5669807, 0.5677849, 0.5701976, 0.56870407, 0.56732535, 0.5736443, 0.5721507, 0.57077205, 0.5715763, 0.5721507, 0.5730699, 0.57192093, 0.57421875, 0.57329965, 0.5744485, 0.5744485, 0.57421875, 0.57295495, 0.5735294, 0.5736443, 0.573989, 0.57421875, 0.57433367, 0.5744485, 0.57410383, 0.57421875, 0.5734145, 0.57433367, 0.57410383, 0.57387406, 0.57421875, 0.57421875, 0.57421875, 0.57433367, 0.57433367, 0.57433367, 0.57421875, 0.5744485, 0.57433367, 0.57410383, 0.57410383, 0.57433367, 0.5746783, 0.57433367, 0.5746783, 0.5744485, 0.57410383, 0.57410383, 0.57513785, 0.57421875]\n",
      " 15%|███████▏                                        | 3/20 [04:36<25:46, 90.94s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Large dropout rate: 0.980932 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C38BE6C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C38BE6C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C38BE6C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247C38BE6C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_3\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_4 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_6 (Bidirectional) (512, 7, 128)        1186304     input_4[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_6[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_15 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_16 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_18 (Lambda)              (2048, 7, 64)        0           dense_15[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_19 (Lambda)              (2048, 7, 64)        0           dense_16[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_21 (Lambda)              (2048, 7, 7)         0           lambda_18[0][0]                                       \n",
      "                                                                 lambda_19[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_3 (Activation)       (2048, 7, 7)         0           lambda_21[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_17 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_6 (Dropout)             (2048, 7, 7)         0           activation_3[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_20 (Lambda)              (2048, 7, 64)        0           dense_17[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_22 (Lambda)              (2048, 7, 64)        0           dropout_6[0][0]                                       \n",
      "                                                                 lambda_20[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_23 (Lambda)              (512, 7, 256)        0           lambda_22[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_3 (TimeDistrib (512, 7, 128)        32896       lambda_23[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_7 (Dropout)             (512, 7, 128)        0           time_distributed_3[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_3 (LayerNor (512, 7, 128)        256         dropout_7[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_7 (Bidirectional) (512, 128)           99328       layer_normalization_3[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_7[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            4128        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_19 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,422,306                                                                                                \n",
      "Trainable params: 1,421,794                                                                                            \n",
      "Non-trainable params: 512                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.49051338, 0.54436386, 0.55719864, 0.55747765, 0.5560826, 0.55747765, 0.5535714, 0.5549665, 0.5535714, 0.5555245, 0.5555245, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5555245, 0.5549665, 0.5541295, 0.55691963, 0.5655692, 0.5733817, 0.5717076, 0.58203125, 0.6063058, 0.6104911, 0.61467636, 0.61997765, 0.6032366, 0.609933, 0.62081474, 0.62862724, 0.6579241, 0.6688058, 0.6766183, 0.67522323, 0.6799665, 0.6026786, 0.6082589, 0.6325335, 0.65094864, 0.6632255, 0.67717636, 0.6738281, 0.6936384, 0.6802455, 0.6961495, 0.6953125, 0.70591515, 0.70842636, 0.7101005, 0.71344864, 0.70870537, 0.703962, 0.71651787, 0.72321427, 0.70731026, 0.7218192, 0.72405136, 0.7257255, 0.7101005, 0.73018974, 0.72126114, 0.7246094, 0.7363281, 0.734654, 0.72935265, 0.734654, 0.7329799, 0.7410714, 0.734654, 0.7405134, 0.7419085, 0.73018974, 0.7427455, 0.7430245, 0.73828125, 0.73214287, 0.7416295, 0.73856026, 0.73688614, 0.7416295, 0.734654, 0.7254464, 0.7128906, 0.7251674, 0.73102677, 0.69866073, 0.70535713, 0.71372765, 0.73800224, 0.702846, 0.6891741, 0.72237724, 0.7329799, 0.734375]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4632353, 0.4777114, 0.50137866, 0.5076976, 0.5132123, 0.5087316, 0.5127528, 0.5236673, 0.5175781, 0.51723343, 0.51953125, 0.5383732, 0.5252757, 0.5366498, 0.53331804, 0.5390625, 0.54308367, 0.5537684, 0.5535386, 0.55422795, 0.55951285, 0.56192553, 0.5603171, 0.5704274, 0.5700827, 0.56893384, 0.5727252, 0.57513785, 0.5790441, 0.5844439, 0.5971967, 0.6105239, 0.6108686, 0.6150046, 0.63143384, 0.6377528, 0.6549862, 0.6638327, 0.67670035, 0.6852022, 0.6981847, 0.6719899, 0.61420035, 0.6482077, 0.6637178, 0.6765855, 0.6873851, 0.70174634, 0.7167969, 0.7306985, 0.7298943, 0.7378217, 0.7485064, 0.7506893, 0.74942553, 0.75057447, 0.7574678, 0.75482535, 0.7622932, 0.7627528, 0.7680377, 0.7657399, 0.76826745, 0.7767693, 0.7733226, 0.7783778, 0.77424175, 0.7732077, 0.77987134, 0.7856158, 0.7868796, 0.78527117, 0.7871094, 0.78963697, 0.7977941, 0.7969899, 0.78768384, 0.7956112, 0.79044116, 0.796875, 0.7892923, 0.79894304, 0.7966452, 0.80089617, 0.7991728, 0.79641545, 0.79848343, 0.80204505, 0.7621783, 0.7815947, 0.79113054, 0.77389705, 0.74643844, 0.7690717, 0.7866498, 0.78963697, 0.75838697, 0.77860755, 0.78860295, 0.79308367]\n",
      " 20%|█████████▌                                      | 4/20 [06:03<23:57, 89.83s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247DB542FC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247DB542FC8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247DB542FC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247DB542FC8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_4\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_5 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_8 (Bidirectional) (512, 7, 256)        2438144     input_5[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_8[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_20 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_21 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_24 (Lambda)              (1536, 7, 64)        0           dense_20[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_25 (Lambda)              (1536, 7, 64)        0           dense_21[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_27 (Lambda)              (1536, 7, 7)         0           lambda_24[0][0]                                       \n",
      "                                                                 lambda_25[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_4 (Activation)       (1536, 7, 7)         0           lambda_27[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_22 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_8 (Dropout)             (1536, 7, 7)         0           activation_4[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_26 (Lambda)              (1536, 7, 64)        0           dense_22[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_28 (Lambda)              (1536, 7, 64)        0           dropout_8[0][0]                                       \n",
      "                                                                 lambda_26[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_29 (Lambda)              (512, 7, 192)        0           lambda_28[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_4 (TimeDistrib (512, 7, 32)         6176        lambda_29[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_9 (Dropout)             (512, 7, 32)         0           time_distributed_4[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_4 (LayerNor (512, 7, 32)         64          dropout_9[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_9 (Bidirectional) (512, 256)           165888      layer_normalization_4[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_9[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            8224        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_24 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,768,066                                                                                                \n",
      "Trainable params: 2,767,042                                                                                            \n",
      "Non-trainable params: 1,024                                                                                            \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5292969, 0.5359933, 0.55078125, 0.5560826, 0.5560826, 0.55691963, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5560826, 0.56529015, 0.5700335, 0.5873326, 0.6171875, 0.6336495, 0.65373886, 0.6729911, 0.6788505, 0.69308037, 0.69754463, 0.702846, 0.7207031, 0.71344864, 0.72377235, 0.72377235, 0.7248884, 0.72935265, 0.719029, 0.73186386, 0.7296317, 0.7167969, 0.7354911, 0.73967636, 0.7273995, 0.73102677, 0.7488839, 0.73828125, 0.7564174, 0.75251114, 0.75, 0.73939735, 0.7516741, 0.74637276, 0.7589286, 0.7483259, 0.74748886, 0.7566964, 0.749721, 0.7516741, 0.7564174, 0.7530692, 0.7586495, 0.76060265, 0.74609375, 0.76311386, 0.7675781, 0.750558, 0.7608817, 0.749163, 0.7416295, 0.7586495, 0.7583705, 0.76227677, 0.75362724, 0.76004463, 0.7564174, 0.76032364, 0.7486049, 0.7513951, 0.75558037, 0.7558594, 0.76143974, 0.74776787, 0.7547433, 0.75362724, 0.7519531, 0.75334823, 0.76311386, 0.75502235, 0.7566964, 0.7580915, 0.7530692, 0.75362724, 0.7589286, 0.7586495, 0.7480469, 0.76199776, 0.75418526, 0.7569755, 0.7569755, 0.7592076, 0.75530136]\n",
      "Train acc of epoch:                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50379133, 0.50919116, 0.5209099, 0.5179228, 0.515625, 0.5245864, 0.5365349, 0.5419347, 0.53952205, 0.53963697, 0.55526197, 0.56169575, 0.5680147, 0.56629133, 0.565602, 0.5744485, 0.57410383, 0.5835248, 0.5950138, 0.59547335, 0.6170726, 0.6193704, 0.6299403, 0.6477482, 0.6631434, 0.68485755, 0.6980699, 0.7209329, 0.72828585, 0.7367877, 0.74276197, 0.74574906, 0.7543658, 0.7509191, 0.7519531, 0.7684972, 0.75953585, 0.7681526, 0.7699908, 0.76424634, 0.7655101, 0.76861215, 0.7658548, 0.7714844, 0.7811351, 0.77722883, 0.7811351, 0.7766544, 0.7815947, 0.7794118, 0.77964157, 0.778148, 0.7821691, 0.78423715, 0.7815947, 0.7830882, 0.7759651, 0.7866498, 0.7794118, 0.7867647, 0.7867647, 0.79067093, 0.78745407, 0.79388785, 0.78607535, 0.7792969, 0.7877987, 0.782284, 0.7949219, 0.7931985, 0.78952205, 0.79480696, 0.7877987, 0.78768384, 0.7887178, 0.7863051, 0.78170955, 0.78642005, 0.7872243, 0.78205425, 0.78125, 0.7845818, 0.78504133, 0.785386, 0.7859605, 0.7837776, 0.7863051, 0.780216, 0.78515625, 0.7857307, 0.7834329, 0.79308367, 0.7887178, 0.79285383, 0.7903263, 0.78389245, 0.7758502, 0.78733915, 0.78860295, 0.7883732]\n",
      " 25%|████████████                                    | 5/20 [07:43<23:14, 92.94s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247E243B448>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247E243B448>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247E243B448>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247E243B448>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_5\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_6 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_10 (Bidirectional (512, 7, 128)        1186304     input_6[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_10[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_25 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_26 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_30 (Lambda)              (2048, 7, 64)        0           dense_25[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_31 (Lambda)              (2048, 7, 64)        0           dense_26[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_33 (Lambda)              (2048, 7, 7)         0           lambda_30[0][0]                                       \n",
      "                                                                 lambda_31[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_5 (Activation)       (2048, 7, 7)         0           lambda_33[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_27 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_10 (Dropout)            (2048, 7, 7)         0           activation_5[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_32 (Lambda)              (2048, 7, 64)        0           dense_27[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_34 (Lambda)              (2048, 7, 64)        0           dropout_10[0][0]                                      \n",
      "                                                                 lambda_32[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_35 (Lambda)              (512, 7, 256)        0           lambda_34[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_5 (TimeDistrib (512, 7, 32)         8224        lambda_35[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_11 (Dropout)            (512, 7, 32)         0           time_distributed_5[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_5 (LayerNor (512, 7, 32)         64          dropout_11[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional_11 (Bidirectional (512, 256)           165888      layer_normalization_5[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_11[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_29 (Dense)                (512, 2)             514         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,460,834                                                                                                \n",
      "Trainable params: 1,460,066                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.54910713, 0.55245537, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5566406, 0.5608259, 0.5638951, 0.5767299, 0.5764509, 0.594308, 0.59375, 0.60574776, 0.5984933, 0.624163, 0.6046317, 0.6110491, 0.62109375, 0.6230469, 0.625558, 0.6238839, 0.61997765, 0.6266741, 0.625279, 0.6157924, 0.6124442, 0.6163505, 0.6238839, 0.625837, 0.6358817, 0.6316964, 0.6314174, 0.6395089, 0.64620537, 0.64397323, 0.6325335, 0.6484375, 0.65234375, 0.64592636, 0.65066963, 0.6470424, 0.6495536, 0.6436942, 0.640346, 0.64453125, 0.64425224, 0.6392299, 0.64620537, 0.6420201, 0.6545759, 0.6422991, 0.65401787, 0.657087, 0.6386719, 0.6392299, 0.6395089, 0.6489955, 0.640904, 0.6473214, 0.6489955, 0.6386719, 0.64508927, 0.6420201, 0.64453125, 0.6476005, 0.657087, 0.65373886, 0.655692, 0.6621094, 0.64508927, 0.6481585, 0.65262276, 0.66155136, 0.66071427, 0.6515067, 0.6621094, 0.66015625, 0.65178573, 0.6489955, 0.6573661, 0.65178573]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5012638, 0.50390625, 0.49712777, 0.5180377, 0.5106847, 0.50700825, 0.5058594, 0.52826285, 0.5232077, 0.53860295, 0.5466452, 0.55526197, 0.5545726, 0.5580193, 0.5622702, 0.565602, 0.5670956, 0.56732535, 0.5606618, 0.5691636, 0.56732535, 0.56824446, 0.5728401, 0.57203585, 0.56824446, 0.57513785, 0.57295495, 0.5777803, 0.57513785, 0.5784697, 0.579159, 0.5777803, 0.5806526, 0.59214157, 0.5857077, 0.5891544, 0.5944393, 0.5969669, 0.5952436, 0.6039752, 0.6121324, 0.6005285, 0.6130515, 0.6139706, 0.6092601, 0.6097197, 0.6143153, 0.6043199, 0.6091452, 0.6169577, 0.61201745, 0.6221278, 0.623966, 0.61799175, 0.62074906, 0.6162684, 0.6153493, 0.6183364, 0.6240809, 0.62339157, 0.6191406, 0.6277574, 0.6190257, 0.6247702, 0.63223803, 0.623966, 0.6305147, 0.62454045, 0.61672795, 0.62442553, 0.6253447, 0.6216682, 0.63120407, 0.63453585, 0.6283318, 0.6282169, 0.6295956, 0.62925094, 0.6337316, 0.6382123, 0.6337316, 0.6297105, 0.6353401, 0.64269304, 0.6383272, 0.63660383, 0.6372932, 0.6384421, 0.6377528, 0.6430377, 0.6360294, 0.63108915, 0.6272978, 0.62879133, 0.6274127, 0.6325827, 0.6284467, 0.6391314, 0.6409697, 0.6349954]\n",
      " 30%|██████████████▍                                 | 6/20 [09:12<21:23, 91.71s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F6DB64C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F6DB64C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F6DB64C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F6DB64C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_6\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_7 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_12 (Bidirectional (512, 7, 128)        1186304     input_7[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_12[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_30 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_31 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_36 (Lambda)              (1536, 7, 64)        0           dense_30[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_37 (Lambda)              (1536, 7, 64)        0           dense_31[0][0]                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "lambda_39 (Lambda)              (1536, 7, 7)         0           lambda_36[0][0]                                       \n",
      "                                                                 lambda_37[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_6 (Activation)       (1536, 7, 7)         0           lambda_39[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_32 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_12 (Dropout)            (1536, 7, 7)         0           activation_6[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_38 (Lambda)              (1536, 7, 64)        0           dense_32[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_40 (Lambda)              (1536, 7, 64)        0           dropout_12[0][0]                                      \n",
      "                                                                 lambda_38[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_41 (Lambda)              (512, 7, 192)        0           lambda_40[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_6 (TimeDistrib (512, 7, 128)        24704       lambda_41[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_13 (Dropout)            (512, 7, 128)        0           time_distributed_6[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_6 (LayerNor (512, 7, 128)        256         dropout_13[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_13 (Bidirectional (512, 256)           264192      layer_normalization_6[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_13[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 16)            4112        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 16)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_34 (Dense)                (512, 2)             34          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,554,866                                                                                                \n",
      "Trainable params: 1,554,098                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.531808, 0.54129463, 0.5535714, 0.5527344, 0.5552455, 0.5549665, 0.5552455, 0.5552455, 0.5552455, 0.5546875, 0.5563616, 0.5636161, 0.578683, 0.61216515, 0.6233259, 0.64481026, 0.6375558, 0.66964287, 0.6799665, 0.68331474, 0.672154, 0.69308037, 0.703125, 0.6953125, 0.70089287, 0.70647323, 0.7123326, 0.7109375, 0.6947545, 0.703125, 0.6891741, 0.7045201, 0.703125, 0.717913, 0.7173549, 0.72293526, 0.72321427, 0.719029, 0.7313058, 0.7126116, 0.70703125, 0.72209823, 0.7117745, 0.72907364, 0.71623886, 0.7123326, 0.7279576, 0.7167969, 0.72098213, 0.72237724, 0.72433037, 0.7050781, 0.7176339, 0.719029, 0.7106585, 0.7123326, 0.7170759, 0.7167969, 0.71623886, 0.7128906, 0.72154015, 0.73046875, 0.718192, 0.7198661, 0.72098213, 0.7271205, 0.7262835, 0.7176339, 0.7109375, 0.71623886, 0.7201451, 0.71595985, 0.719029, 0.718192, 0.72377235, 0.7360491, 0.72321427, 0.71428573, 0.72237724, 0.7112165, 0.71316963, 0.7170759, 0.72237724, 0.71372765, 0.7089844, 0.72321427, 0.7248884, 0.72321427, 0.72098213, 0.72237724, 0.7279576, 0.72154015, 0.71372765, 0.718471, 0.7198661, 0.72154015, 0.72907364, 0.72321427, 0.71428573, 0.72321427]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5050551, 0.5184972, 0.52745867, 0.53550094, 0.5344669, 0.5403263, 0.5419347, 0.56548715, 0.563534, 0.5730699, 0.58191633, 0.5969669, 0.60673255, 0.6340763, 0.6417739, 0.65567553, 0.6633732, 0.6784237, 0.6903722, 0.7003676, 0.7007123, 0.715648, 0.71231616, 0.7164522, 0.71920955, 0.71920955, 0.72495407, 0.7221967, 0.7264476, 0.72598803, 0.7372472, 0.72035843, 0.72575825, 0.74161303, 0.73173255, 0.7409237, 0.7344899, 0.7400046, 0.738511, 0.7309283, 0.7421875, 0.7250689, 0.7356388, 0.7390855, 0.7432215, 0.7380515, 0.7358686, 0.7414982, 0.74046415, 0.7369026, 0.7409237, 0.735409, 0.73357075, 0.7396599, 0.7332261, 0.74862134, 0.7375919, 0.7400046, 0.7497702, 0.74080884, 0.73954505, 0.7403493, 0.74574906, 0.7410386, 0.7412684, 0.7455193, 0.74299175, 0.7432215, 0.7440257, 0.7398897, 0.74735755, 0.7400046, 0.7434513, 0.745864, 0.74011946, 0.7421875, 0.74172795, 0.7394301, 0.7410386, 0.7474724, 0.7467831, 0.7402344, 0.74919575, 0.7365579, 0.7378217, 0.73644304, 0.7449449, 0.74597883, 0.7410386, 0.7378217, 0.7403493, 0.7485064, 0.7389706, 0.7378217, 0.7528722, 0.7466682, 0.7441406, 0.74425554, 0.7487362, 0.75298715]\n",
      " 35%|████████████████▊                               | 7/20 [10:44<19:54, 91.89s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F78DF608>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F78DF608>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F78DF608>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000247F78DF608>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_7\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_8 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_14 (Bidirectional (512, 7, 64)         584960      input_8[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 64)         256         bidirectional_14[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 64)         0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_35 (Dense)                (512, 7, 256)        16384       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_36 (Dense)                (512, 7, 256)        16384       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_42 (Lambda)              (2048, 7, 64)        0           dense_35[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_43 (Lambda)              (2048, 7, 64)        0           dense_36[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_45 (Lambda)              (2048, 7, 7)         0           lambda_42[0][0]                                       \n",
      "                                                                 lambda_43[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_7 (Activation)       (2048, 7, 7)         0           lambda_45[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_37 (Dense)                (512, 7, 256)        16384       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_14 (Dropout)            (2048, 7, 7)         0           activation_7[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_44 (Lambda)              (2048, 7, 64)        0           dense_37[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_46 (Lambda)              (2048, 7, 64)        0           dropout_14[0][0]                                      \n",
      "                                                                 lambda_44[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_47 (Lambda)              (512, 7, 256)        0           lambda_46[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_7 (TimeDistrib (512, 7, 128)        32896       lambda_47[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_15 (Dropout)            (512, 7, 128)        0           time_distributed_7[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_7 (LayerNor (512, 7, 128)        256         dropout_15[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_15 (Bidirectional (512, 64)            41472       layer_normalization_7[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 64)            256         bidirectional_15[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 64)            0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_39 (Dense)                (512, 2)             130         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 709,378                                                                                                  \n",
      "Trainable params: 709,122                                                                                              \n",
      "Non-trainable params: 256                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.55803573, 0.56584823, 0.5867745, 0.6171875, 0.6328125, 0.6621094, 0.6782924, 0.6810826, 0.6796875, 0.6983817, 0.7101005, 0.7273995, 0.70647323, 0.7114955, 0.718192, 0.72879463, 0.7416295, 0.73214287, 0.719587, 0.7430245, 0.74665177, 0.73883927, 0.7424665, 0.7441406, 0.72879463, 0.7363281, 0.73716515, 0.7427455, 0.7407924, 0.7357701, 0.749442, 0.74637276, 0.73828125, 0.73074776, 0.73856026, 0.73856026, 0.749163, 0.75, 0.749442, 0.7564174, 0.7402344, 0.75418526, 0.7488839, 0.74581474, 0.7276786, 0.7427455, 0.75446427, 0.74609375, 0.7530692, 0.7558594, 0.74748886, 0.75558037, 0.7530692, 0.749442, 0.7402344, 0.7424665, 0.750279, 0.750837, 0.7469308, 0.7421875, 0.7469308, 0.7391183, 0.75334823, 0.75362724, 0.7530692, 0.7594866, 0.73772323, 0.73967636, 0.74581474, 0.7419085, 0.7469308, 0.75279015, 0.74637276, 0.75, 0.74581474, 0.7572545, 0.7566964, 0.7516741, 0.75279015, 0.749442, 0.7452567, 0.7480469, 0.74553573, 0.7435826, 0.75279015, 0.7575335, 0.7430245, 0.74748886, 0.7363281, 0.73995537, 0.7421875, 0.74609375, 0.73856026, 0.7354911, 0.73856026, 0.749163, 0.75418526, 0.7586495, 0.7441406, 0.750558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc of epoch:                                                                                                    \n",
      "[0.5124081, 0.54204965, 0.5622702, 0.5940947, 0.6251149, 0.6609605, 0.6907169, 0.70714617, 0.73138785, 0.74080884, 0.7502298, 0.7641314, 0.7675781, 0.7866498, 0.7759651, 0.78986675, 0.7888327, 0.79262406, 0.7934283, 0.79549634, 0.7921645, 0.8005515, 0.80066633, 0.80388325, 0.8039982, 0.8106618, 0.80893844, 0.8036535, 0.808364, 0.81307447, 0.81548715, 0.8147978, 0.8058364, 0.8021599, 0.8059513, 0.80951285, 0.80893844, 0.8127298, 0.81606156, 0.8226103, 0.81410843, 0.8152574, 0.82387406, 0.8169807, 0.81732535, 0.8121553, 0.8184743, 0.8181296, 0.8234145, 0.8228401, 0.81824446, 0.8266314, 0.8188189, 0.8305377, 0.82651657, 0.81893384, 0.8190487, 0.8193934, 0.82755053, 0.81606156, 0.81675094, 0.8146829, 0.8258272, 0.8172105, 0.8105469, 0.81973803, 0.82674634, 0.82111675, 0.8222656, 0.82077205, 0.8158318, 0.81514245, 0.81858915, 0.82640165, 0.8143382, 0.82238054, 0.8206572, 0.8143382, 0.8303079, 0.82674634, 0.82605696, 0.82605696, 0.8222656, 0.8215763, 0.8230699, 0.82732075, 0.8221507, 0.82892925, 0.8259421, 0.81824446, 0.8178998, 0.8177849, 0.8201976, 0.8210018, 0.8234145, 0.8277803, 0.8199678, 0.8224954, 0.8181296, 0.82433367]\n",
      " 40%|███████████████████▏                            | 8/20 [12:08<17:53, 89.50s/trial, best loss: -0.7712053656578064]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024807425B08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024807425B08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024807425B08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024807425B08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_8\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_9 (InputLayer)            [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_16 (Bidirectional (512, 7, 256)        2438144     input_9[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_16[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_40 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_41 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_48 (Lambda)              (1536, 7, 64)        0           dense_40[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_49 (Lambda)              (1536, 7, 64)        0           dense_41[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_51 (Lambda)              (1536, 7, 7)         0           lambda_48[0][0]                                       \n",
      "                                                                 lambda_49[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_8 (Activation)       (1536, 7, 7)         0           lambda_51[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_42 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_16 (Dropout)            (1536, 7, 7)         0           activation_8[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_50 (Lambda)              (1536, 7, 64)        0           dense_42[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_52 (Lambda)              (1536, 7, 64)        0           dropout_16[0][0]                                      \n",
      "                                                                 lambda_50[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_53 (Lambda)              (512, 7, 192)        0           lambda_52[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_8 (TimeDistrib (512, 7, 64)         12352       lambda_53[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_17 (Dropout)            (512, 7, 64)         0           time_distributed_8[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_8 (LayerNor (512, 7, 64)         128         dropout_17[0][0]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_17 (Bidirectional (512, 128)           66560       layer_normalization_8[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_17[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_44 (Dense)                (512, 2)             258         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,666,434                                                                                                \n",
      "Trainable params: 2,665,666                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.57421875, 0.5795201, 0.6079799, 0.640625, 0.6489955, 0.68387276, 0.69196427, 0.69893974, 0.69921875, 0.7268415, 0.7204241, 0.7271205, 0.733817, 0.733817, 0.7578125, 0.75418526, 0.750279, 0.7265625, 0.74441963, 0.75530136, 0.73772323, 0.7452567, 0.750279, 0.7176339, 0.75502235, 0.76339287, 0.7564174, 0.7569755, 0.75362724, 0.7592076, 0.7583705, 0.7430245, 0.750837, 0.75251114, 0.750279, 0.7580915, 0.74748886, 0.75251114, 0.75558037, 0.76283485, 0.7586495, 0.765904, 0.7739955, 0.7608817, 0.76143974, 0.7675781, 0.7742745, 0.76953125, 0.7592076, 0.77092636, 0.75418526, 0.76004463, 0.75530136, 0.76841515, 0.766462, 0.7594866, 0.7586495, 0.7645089, 0.76060265, 0.75530136, 0.76813614, 0.7569755, 0.76813614, 0.76060265, 0.75502235, 0.76283485, 0.7639509, 0.76953125, 0.77120537, 0.7714844, 0.76813614, 0.76813614, 0.7645089, 0.76925224, 0.7670201, 0.76283485, 0.7723214, 0.77008927, 0.76143974, 0.765346, 0.7675781, 0.7645089, 0.77008927, 0.7642299, 0.7734375, 0.77566963, 0.7792969, 0.76981026, 0.765904, 0.76981026, 0.7723214, 0.76339287, 0.7670201, 0.76813614, 0.76785713, 0.7580915, 0.7720424, 0.7714844, 0.77092636, 0.7672991]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.52389705, 0.54630053, 0.5806526, 0.6252298, 0.6686581, 0.7169118, 0.7450597, 0.75838697, 0.768727, 0.7835478, 0.7909007, 0.7931985, 0.8021599, 0.80411303, 0.8057215, 0.8118107, 0.82238054, 0.8196232, 0.8159467, 0.82111675, 0.8139936, 0.81870407, 0.8246783, 0.813534, 0.8206572, 0.81858915, 0.8190487, 0.8249081, 0.82651657, 0.8269761, 0.8218061, 0.8215763, 0.8227252, 0.8249081, 0.8308824, 0.82640165, 0.8257123, 0.8212316, 0.8201976, 0.81410843, 0.82387406, 0.81870407, 0.82858455, 0.8228401, 0.82192093, 0.8261719, 0.82674634, 0.8255974, 0.828125, 0.82973343, 0.8227252, 0.8235294, 0.8278952, 0.8234145, 0.82869947, 0.82456344, 0.82433367, 0.828125, 0.82950366, 0.82674634, 0.82421875, 0.8252528, 0.82892925, 0.8349035, 0.82858455, 0.8296186, 0.8346737, 0.8308824, 0.8290441, 0.8274357, 0.8340993, 0.82973343, 0.82950366, 0.8328355, 0.82766545, 0.8334099, 0.83295035, 0.82892925, 0.8308824, 0.82858455, 0.83203125, 0.8269761, 0.8300781, 0.8292739, 0.8340993, 0.8334099, 0.828125, 0.83432907, 0.82858455, 0.83237594, 0.83019304, 0.8357077, 0.83513325, 0.8346737, 0.8350184, 0.83237594, 0.83019304, 0.82548255, 0.8378906, 0.82984835]\n",
      " 45%|████████████████████████▊                              | 9/20 [13:49<17:00, 92.80s/trial, best loss: -0.779296875]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480C053508>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480C053508>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480C053508>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480C053508>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_9\"                                                                                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_10 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_18 (Bidirectional (512, 7, 128)        1186304     input_10[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_18[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_45 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_46 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_54 (Lambda)              (2048, 7, 64)        0           dense_45[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_55 (Lambda)              (2048, 7, 64)        0           dense_46[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_57 (Lambda)              (2048, 7, 7)         0           lambda_54[0][0]                                       \n",
      "                                                                 lambda_55[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_9 (Activation)       (2048, 7, 7)         0           lambda_57[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_47 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_18 (Dropout)            (2048, 7, 7)         0           activation_9[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_56 (Lambda)              (2048, 7, 64)        0           dense_47[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_58 (Lambda)              (2048, 7, 64)        0           dropout_18[0][0]                                      \n",
      "                                                                 lambda_56[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_59 (Lambda)              (512, 7, 256)        0           lambda_58[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_9 (TimeDistrib (512, 7, 32)         8224        lambda_59[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_19 (Dropout)            (512, 7, 32)         0           time_distributed_9[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_9 (LayerNor (512, 7, 32)         64          dropout_19[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_19 (Bidirectional (512, 64)            16896       layer_normalization_9[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 64)            256         bidirectional_19[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 64)            0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 16)            1040        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 16)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_49 (Dense)                (512, 2)             34          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,311,634                                                                                                \n",
      "Trainable params: 1,311,250                                                                                            \n",
      "Non-trainable params: 384                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.53376114, 0.55998886, 0.5566406, 0.5605469, 0.5560826, 0.5563616, 0.5552455, 0.5546875, 0.5552455, 0.5563616, 0.5555245, 0.5549665, 0.5552455, 0.5544085, 0.5560826, 0.5555245, 0.55747765, 0.5594308, 0.5672433, 0.5797991, 0.59151787, 0.6141183, 0.6233259, 0.6478795, 0.6646205, 0.6643415, 0.66936386, 0.6827567, 0.68415177, 0.69140625, 0.6922433, 0.6964286, 0.703125, 0.702567, 0.7120536, 0.70814735, 0.7128906, 0.7128906, 0.71595985, 0.7112165, 0.71428573, 0.71456474, 0.7042411, 0.6953125, 0.702846, 0.71540177, 0.7050781, 0.7114955, 0.7265625, 0.719308, 0.71512276, 0.7279576, 0.70089287, 0.7329799, 0.7296317, 0.7112165, 0.71512276, 0.71372765, 0.7207031, 0.70842636, 0.7120536, 0.72377235, 0.72293526, 0.73046875, 0.735212, 0.718192, 0.719587, 0.7207031, 0.7140067, 0.719029, 0.7234933, 0.7391183, 0.72237724, 0.72293526, 0.7271205, 0.7265625, 0.718192, 0.7218192, 0.7126116, 0.7268415, 0.7257255, 0.7156808, 0.71316963, 0.72405136, 0.7207031, 0.72293526, 0.72154015, 0.7279576, 0.7198661, 0.72321427, 0.7167969, 0.7120536, 0.72935265, 0.7262835, 0.7276786, 0.735212, 0.7273995, 0.7271205, 0.734375, 0.73018974]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.47759652, 0.4921875, 0.51183367, 0.52079505, 0.5132123, 0.51918656, 0.5210248, 0.5249311, 0.53079045, 0.52745867, 0.5381434, 0.5391774, 0.54480696, 0.5441176, 0.55641085, 0.5623851, 0.57513785, 0.5761719, 0.5877757, 0.5957031, 0.6092601, 0.6169577, 0.6394761, 0.6515395, 0.6613051, 0.6786535, 0.6794577, 0.6907169, 0.6996783, 0.70358455, 0.6977252, 0.7164522, 0.714614, 0.7256434, 0.72495407, 0.7250689, 0.7280561, 0.730239, 0.72483915, 0.7363281, 0.7369026, 0.7288603, 0.74057907, 0.7387408, 0.7319623, 0.7436811, 0.7412684, 0.7480469, 0.7450597, 0.7474724, 0.7480469, 0.7465533, 0.7471278, 0.74276197, 0.7456342, 0.7514936, 0.7480469, 0.73816633, 0.7420726, 0.7471278, 0.7434513, 0.7493107, 0.7528722, 0.75574446, 0.7463235, 0.7524127, 0.74701285, 0.7506893, 0.7534467, 0.7521829, 0.75057447, 0.75827205, 0.75057447, 0.7534467, 0.75172335, 0.7552849, 0.7527574, 0.7496553, 0.7471278, 0.753102, 0.7536765, 0.7575827, 0.74574906, 0.74597883, 0.7525276, 0.7444853, 0.7552849, 0.7533318, 0.7524127, 0.7590763, 0.7485064, 0.74942553, 0.75206804, 0.75608915, 0.75827205, 0.7545956, 0.7574678, 0.7512638, 0.7433364, 0.75827205]\n",
      " 50%|███████████████████████████                           | 10/20 [15:21<15:27, 92.74s/trial, best loss: -0.779296875]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024808FB2288>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024808FB2288>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024808FB2288>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024808FB2288>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_10\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_11 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_20 (Bidirectional (512, 7, 256)        2438144     input_11[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_20[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_50 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_51 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_60 (Lambda)              (2048, 7, 64)        0           dense_50[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_61 (Lambda)              (2048, 7, 64)        0           dense_51[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_63 (Lambda)              (2048, 7, 7)         0           lambda_60[0][0]                                       \n",
      "                                                                 lambda_61[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_10 (Activation)      (2048, 7, 7)         0           lambda_63[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_52 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_20 (Dropout)            (2048, 7, 7)         0           activation_10[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_62 (Lambda)              (2048, 7, 64)        0           dense_52[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_64 (Lambda)              (2048, 7, 64)        0           dropout_20[0][0]                                      \n",
      "                                                                 lambda_62[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_65 (Lambda)              (512, 7, 256)        0           lambda_64[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_10 (TimeDistri (512, 7, 32)         8224        lambda_65[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_21 (Dropout)            (512, 7, 32)         0           time_distributed_10[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_10 (LayerNo (512, 7, 32)         64          dropout_21[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_21 (Bidirectional (512, 128)           50176       layer_normalization_10[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_21[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 8)             1032        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 8)             0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_54 (Dense)                (512, 2)             18          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,695,802                                                                                                \n",
      "Trainable params: 2,695,034                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5613839, 0.58928573, 0.58203125, 0.59598213, 0.6026786, 0.6347656, 0.6576451, 0.64341515, 0.6582031, 0.66629463, 0.6886161, 0.6738281, 0.6861049, 0.68415177, 0.70061386, 0.6972656, 0.6939174, 0.73074776, 0.73102677, 0.72377235, 0.7363281, 0.7452567, 0.7271205, 0.7486049, 0.7419085, 0.749163, 0.7410714, 0.73772323, 0.7421875, 0.7480469, 0.73856026, 0.7419085, 0.70703125, 0.7268415, 0.7438616, 0.73102677, 0.73074776, 0.74776787, 0.75362724, 0.733817, 0.74720985, 0.7433036, 0.7566964, 0.7530692, 0.7625558, 0.75251114, 0.75223213, 0.75334823, 0.7578125, 0.734654, 0.7513951, 0.75446427, 0.749442, 0.7435826, 0.75502235, 0.76116073, 0.7569755, 0.7558594, 0.7589286, 0.7636719, 0.75418526, 0.76116073, 0.75502235, 0.766183, 0.76199776, 0.7569755, 0.75530136, 0.749721, 0.7572545, 0.749721, 0.74748886, 0.75279015, 0.7580915, 0.7530692, 0.7547433, 0.766462, 0.75418526, 0.74637276, 0.7594866, 0.7402344, 0.7592076, 0.7516741, 0.74748886, 0.7597656, 0.76116073, 0.7435826, 0.7569755, 0.7519531, 0.749442, 0.76032364, 0.7586495, 0.7580915, 0.76311386, 0.7580915, 0.7583705, 0.7513951, 0.75362724, 0.7516741, 0.76199776, 0.7575335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc of epoch:                                                                                                    \n",
      "[0.48540902, 0.51941633, 0.5414752, 0.5788143, 0.610409, 0.6491268, 0.6862362, 0.7143842, 0.7421875, 0.75620407, 0.7644761, 0.7711397, 0.7805607, 0.7909007, 0.7931985, 0.7856158, 0.7902114, 0.7913603, 0.8050322, 0.78952205, 0.8066406, 0.7961857, 0.8027344, 0.79986215, 0.8050322, 0.8088235, 0.8126149, 0.80893844, 0.81767005, 0.81330425, 0.8224954, 0.81410843, 0.809398, 0.8105469, 0.81514245, 0.8172105, 0.81077665, 0.8180147, 0.8174403, 0.81089157, 0.8158318, 0.8150276, 0.813534, 0.8147978, 0.8172105, 0.8188189, 0.8112362, 0.82421875, 0.81870407, 0.81985295, 0.8125, 0.8169807, 0.81606156, 0.81204045, 0.823989, 0.8147978, 0.8191636, 0.8199678, 0.8196232, 0.8172105, 0.81732535, 0.8227252, 0.82238054, 0.8326057, 0.8174403, 0.831227, 0.8161765, 0.823989, 0.8261719, 0.816636, 0.8227252, 0.81629133, 0.82077205, 0.82410383, 0.81767005, 0.8234145, 0.8214614, 0.8230699, 0.8214614, 0.8177849, 0.8278952, 0.82238054, 0.8212316, 0.82203585, 0.83329505, 0.8193934, 0.8181296, 0.82387406, 0.8201976, 0.8175551, 0.82421875, 0.82766545, 0.82766545, 0.8204274, 0.8315717, 0.8221507, 0.8313419, 0.8318015, 0.8252528, 0.82984835]\n",
      " 55%|█████████████████████████████▋                        | 11/20 [17:08<14:31, 96.82s/trial, best loss: -0.779296875]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480E690D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480E690D08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480E690D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002480E690D08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_11\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_12 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_22 (Bidirectional (512, 7, 256)        2438144     input_12[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_22[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_55 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_56 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_66 (Lambda)              (1536, 7, 64)        0           dense_55[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_67 (Lambda)              (1536, 7, 64)        0           dense_56[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_69 (Lambda)              (1536, 7, 7)         0           lambda_66[0][0]                                       \n",
      "                                                                 lambda_67[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_11 (Activation)      (1536, 7, 7)         0           lambda_69[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_57 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_22 (Dropout)            (1536, 7, 7)         0           activation_11[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_68 (Lambda)              (1536, 7, 64)        0           dense_57[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_70 (Lambda)              (1536, 7, 64)        0           dropout_22[0][0]                                      \n",
      "                                                                 lambda_68[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_71 (Lambda)              (512, 7, 192)        0           lambda_70[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_11 (TimeDistri (512, 7, 64)         12352       lambda_71[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_23 (Dropout)            (512, 7, 64)         0           time_distributed_11[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_11 (LayerNo (512, 7, 64)         128         dropout_23[0][0]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_23 (Bidirectional (512, 128)           66560       layer_normalization_11[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_23[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_59 (Dense)                (512, 2)             258         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,666,434                                                                                                \n",
      "Trainable params: 2,665,666                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.561663, 0.5638951, 0.58370537, 0.5845424, 0.593192, 0.61439735, 0.625837, 0.63560265, 0.65904015, 0.66908485, 0.671875, 0.7047991, 0.702288, 0.71456474, 0.72405136, 0.7296317, 0.7329799, 0.7357701, 0.73102677, 0.735212, 0.74441963, 0.74637276, 0.7511161, 0.7578125, 0.74748886, 0.7391183, 0.7572545, 0.750558, 0.7583705, 0.7511161, 0.75362724, 0.74497765, 0.7578125, 0.77008927, 0.7564174, 0.750558, 0.7575335, 0.75530136, 0.7402344, 0.75223213, 0.749163, 0.7583705, 0.75446427, 0.7578125, 0.76116073, 0.7558594, 0.76116073, 0.76199776, 0.75530136, 0.76060265, 0.7608817, 0.77008927, 0.76199776, 0.75418526, 0.7572545, 0.7561384, 0.76199776, 0.76199776, 0.76813614, 0.7642299, 0.75558037, 0.75362724, 0.7670201, 0.76171875, 0.76004463, 0.7586495, 0.749442, 0.7597656, 0.75251114, 0.7625558, 0.76283485, 0.7686942, 0.75279015, 0.7594866, 0.7513951, 0.765625, 0.75, 0.74720985, 0.76060265, 0.76227677, 0.7580915, 0.75446427, 0.7672991, 0.7569755, 0.76060265, 0.76283485, 0.76841515, 0.7578125, 0.7670201, 0.7639509, 0.764788, 0.76283485, 0.7580915, 0.76813614, 0.76311386, 0.76032364, 0.765625, 0.7642299, 0.7642299, 0.7625558]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4977022, 0.50838697, 0.5157399, 0.5221737, 0.54113054, 0.5412454, 0.5634191, 0.5892693, 0.59995407, 0.6343061, 0.6652114, 0.6920956, 0.71920955, 0.73793656, 0.75517005, 0.76619947, 0.77642465, 0.7943474, 0.79951745, 0.795841, 0.79630053, 0.8068704, 0.8097426, 0.8091682, 0.81330425, 0.8077895, 0.8092831, 0.8150276, 0.8144531, 0.8115809, 0.81330425, 0.81870407, 0.8097426, 0.809398, 0.80893844, 0.8122702, 0.81112134, 0.81629133, 0.8123851, 0.81629133, 0.8200827, 0.8144531, 0.811466, 0.8127298, 0.81387866, 0.8110064, 0.8221507, 0.8143382, 0.8102022, 0.8115809, 0.8144531, 0.8115809, 0.8146829, 0.8183594, 0.81985295, 0.8126149, 0.8144531, 0.8050322, 0.81043196, 0.81204045, 0.816636, 0.8136489, 0.81858915, 0.8149127, 0.8204274, 0.8205423, 0.81640625, 0.8137638, 0.81089157, 0.82111675, 0.81767005, 0.81732535, 0.80870867, 0.81870407, 0.8136489, 0.82456344, 0.81422335, 0.80985755, 0.8136489, 0.8157169, 0.8201976, 0.8131893, 0.81858915, 0.8126149, 0.81387866, 0.81548715, 0.8134191, 0.8177849, 0.81077665, 0.8169807, 0.8196232, 0.81307447, 0.8123851, 0.8112362, 0.81985295, 0.8144531, 0.8180147, 0.8097426, 0.8158318, 0.8137638]\n",
      " 60%|████████████████████████████████▍                     | 12/20 [18:50<13:06, 98.32s/trial, best loss: -0.779296875]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815295D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815295D08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815295D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815295D08>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_12\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_13 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_24 (Bidirectional (512, 7, 256)        2438144     input_13[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_24[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_60 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_61 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_72 (Lambda)              (1536, 7, 64)        0           dense_60[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_73 (Lambda)              (1536, 7, 64)        0           dense_61[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_75 (Lambda)              (1536, 7, 7)         0           lambda_72[0][0]                                       \n",
      "                                                                 lambda_73[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_12 (Activation)      (1536, 7, 7)         0           lambda_75[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_62 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_24 (Dropout)            (1536, 7, 7)         0           activation_12[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_74 (Lambda)              (1536, 7, 64)        0           dense_62[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_76 (Lambda)              (1536, 7, 64)        0           dropout_24[0][0]                                      \n",
      "                                                                 lambda_74[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_77 (Lambda)              (512, 7, 192)        0           lambda_76[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_12 (TimeDistri (512, 7, 128)        24704       lambda_77[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_25 (Dropout)            (512, 7, 128)        0           time_distributed_12[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_12 (LayerNo (512, 7, 128)        256         dropout_25[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_25 (Bidirectional (512, 256)           264192      layer_normalization_12[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_25[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 16)            4112        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 16)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_64 (Dense)                (512, 2)             34          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,880,946                                                                                                \n",
      "Trainable params: 2,879,922                                                                                            \n",
      "Non-trainable params: 1,024                                                                                            \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.48158482, 0.5343192, 0.5613839, 0.561663, 0.5613839, 0.55970985, 0.5577567, 0.55719864, 0.5552455, 0.5555245, 0.5558036, 0.5611049, 0.5644531, 0.57449776, 0.57589287, 0.58091515, 0.624721, 0.608817, 0.6157924, 0.63643974, 0.6654576, 0.6646205, 0.66015625, 0.65345985, 0.6704799, 0.67438614, 0.66657364, 0.6699219, 0.6844308, 0.69084823, 0.7123326, 0.70591515, 0.6858259, 0.69252235, 0.69308037, 0.6939174, 0.69308037, 0.67745537, 0.703125, 0.703683, 0.70870537, 0.70591515, 0.7254464, 0.72879463, 0.72321427, 0.72405136, 0.7285156, 0.7218192, 0.718471, 0.71623886, 0.718192, 0.71595985, 0.70814735, 0.69977677, 0.70089287, 0.7248884, 0.72405136, 0.72237724, 0.72433037, 0.7092634, 0.7140067, 0.7140067, 0.7020089, 0.70731026, 0.72154015, 0.72293526, 0.7112165, 0.7098214, 0.6967076, 0.70758927, 0.7170759, 0.73074776, 0.718192, 0.69810265, 0.71484375, 0.7103795, 0.69280136, 0.71484375, 0.703125, 0.6958705, 0.7061942, 0.7173549, 0.7042411, 0.7167969, 0.7047991, 0.72405136, 0.72126114, 0.7201451, 0.71484375, 0.7167969, 0.71344864, 0.7117745, 0.72237724, 0.7276786, 0.735212, 0.7207031, 0.7167969, 0.7276786, 0.7128906, 0.7251674]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4955193, 0.52297795, 0.5275735, 0.53825825, 0.54515165, 0.5537684, 0.5557215, 0.5675551, 0.5699678, 0.5678998, 0.582261, 0.58536303, 0.60098803, 0.6061581, 0.60420495, 0.613511, 0.6228171, 0.63338697, 0.6324678, 0.6462546, 0.64981616, 0.6622243, 0.68141085, 0.67451745, 0.6799173, 0.68393844, 0.6775046, 0.68106616, 0.6876149, 0.6830193, 0.6964614, 0.69232535, 0.6994485, 0.6930147, 0.7008272, 0.6918658, 0.70013785, 0.69267005, 0.70255053, 0.7112822, 0.7022059, 0.70519304, 0.70048255, 0.7018612, 0.7155331, 0.71231616, 0.70358455, 0.7097886, 0.70611215, 0.7153033, 0.71668196, 0.7185202, 0.71047795, 0.7038143, 0.70795035, 0.71449906, 0.7176011, 0.7162224, 0.7162224, 0.71714157, 0.7158778, 0.7138097, 0.7080653, 0.7105928, 0.7190947, 0.7076057, 0.7110524, 0.70932907, 0.72357535, 0.7200138, 0.71254593, 0.71357995, 0.73173255, 0.7174862, 0.7215074, 0.7221967, 0.71139705, 0.70829505, 0.7202436, 0.71817553, 0.7216222, 0.71737134, 0.7119715, 0.72035843, 0.7094439, 0.7202436, 0.7221967, 0.7136949, 0.7221967, 0.71794575, 0.70932907, 0.7185202, 0.7180607, 0.71576285, 0.7240349, 0.71920955, 0.715648, 0.72357535, 0.7221967, 0.7209329]\n",
      " 65%|██████████████████████████████████▍                  | 13/20 [20:39<11:52, 101.80s/trial, best loss: -0.779296875]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815262C48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815262C48>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815262C48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x0000024815262C48>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_13\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_14 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_26 (Bidirectional (512, 7, 128)        1186304     input_14[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_26[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_65 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_66 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_78 (Lambda)              (1536, 7, 64)        0           dense_65[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_79 (Lambda)              (1536, 7, 64)        0           dense_66[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_81 (Lambda)              (1536, 7, 7)         0           lambda_78[0][0]                                       \n",
      "                                                                 lambda_79[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_13 (Activation)      (1536, 7, 7)         0           lambda_81[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_67 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_26 (Dropout)            (1536, 7, 7)         0           activation_13[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_80 (Lambda)              (1536, 7, 64)        0           dense_67[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_82 (Lambda)              (1536, 7, 64)        0           dropout_26[0][0]                                      \n",
      "                                                                 lambda_80[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_83 (Lambda)              (512, 7, 192)        0           lambda_82[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_13 (TimeDistri (512, 7, 128)        24704       lambda_83[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_27 (Dropout)            (512, 7, 128)        0           time_distributed_13[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_13 (LayerNo (512, 7, 128)        256         dropout_27[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_27 (Bidirectional (512, 128)           99328       layer_normalization_13[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_27[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 16)            2064        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 16)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_69 (Dense)                (512, 2)             34          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,387,442                                                                                                \n",
      "Trainable params: 1,386,930                                                                                            \n",
      "Non-trainable params: 512                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5864955, 0.59793526, 0.63643974, 0.6531808, 0.6827567, 0.6863839, 0.6944755, 0.70061386, 0.70870537, 0.70842636, 0.7017299, 0.7156808, 0.7271205, 0.734933, 0.7173549, 0.7234933, 0.7391183, 0.7374442, 0.733538, 0.73660713, 0.7271205, 0.734933, 0.7530692, 0.75446427, 0.75362724, 0.734375, 0.7594866, 0.750558, 0.74720985, 0.74581474, 0.74497765, 0.7421875, 0.7391183, 0.7519531, 0.7424665, 0.75362724, 0.749442, 0.74469864, 0.7589286, 0.74776787, 0.75279015, 0.750837, 0.7586495, 0.7578125, 0.76339287, 0.7511161, 0.76199776, 0.7566964, 0.76813614, 0.749442, 0.75390625, 0.7636719, 0.7438616, 0.7586495, 0.7511161, 0.7519531, 0.75530136, 0.7578125, 0.75502235, 0.75, 0.75502235, 0.76060265, 0.764788, 0.7578125, 0.749721, 0.75418526, 0.76981026, 0.75558037, 0.75446427, 0.7734375, 0.7564174, 0.765346, 0.75502235, 0.75390625, 0.76311386, 0.75362724, 0.780413, 0.7720424, 0.75530136, 0.7566964, 0.76060265, 0.7564174, 0.76785713, 0.765067, 0.76171875, 0.7572545, 0.7642299, 0.750279, 0.7558594, 0.7516741, 0.7569755, 0.75334823, 0.765346, 0.75502235, 0.75558037, 0.76199776, 0.7575335, 0.7636719, 0.75223213, 0.76841515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc of epoch:                                                                                                    \n",
      "[0.5381434, 0.581227, 0.62706804, 0.6690028, 0.7099035, 0.7423024, 0.76160383, 0.7705653, 0.77389705, 0.7736673, 0.7846967, 0.7871094, 0.79067093, 0.7980239, 0.7936581, 0.79848343, 0.8102022, 0.8066406, 0.8059513, 0.80767465, 0.809398, 0.81089157, 0.809398, 0.8136489, 0.81629133, 0.82111675, 0.81514245, 0.8200827, 0.8125, 0.81732535, 0.82421875, 0.8139936, 0.8118107, 0.8172105, 0.81950825, 0.82111675, 0.81456804, 0.81629133, 0.8200827, 0.8161765, 0.816636, 0.8146829, 0.82077205, 0.8255974, 0.81767005, 0.8213465, 0.823989, 0.82192093, 0.8227252, 0.8227252, 0.825023, 0.82421875, 0.8227252, 0.8296186, 0.8258272, 0.8212316, 0.82295495, 0.82410383, 0.8280101, 0.82387406, 0.82640165, 0.816636, 0.8269761, 0.8283548, 0.8300781, 0.82410383, 0.82950366, 0.8288143, 0.82984835, 0.8327206, 0.8300781, 0.8231847, 0.82674634, 0.8326057, 0.8272059, 0.82203585, 0.8274357, 0.8272059, 0.8300781, 0.82548255, 0.82513785, 0.8290441, 0.8174403, 0.8330653, 0.8326057, 0.828125, 0.8308824, 0.8292739, 0.828125, 0.8344439, 0.8266314, 0.8215763, 0.82295495, 0.82456344, 0.8137638, 0.82674634, 0.83191633, 0.83214617, 0.82456344, 0.82892925]\n",
      " 70%|████████████████████████████████▏             | 14/20 [22:16<10:00, 100.12s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481F8D54C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481F8D54C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481F8D54C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481F8D54C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_14\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_15 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_28 (Bidirectional (512, 7, 256)        2438144     input_15[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_28[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_70 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_71 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_84 (Lambda)              (2048, 7, 64)        0           dense_70[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_85 (Lambda)              (2048, 7, 64)        0           dense_71[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_87 (Lambda)              (2048, 7, 7)         0           lambda_84[0][0]                                       \n",
      "                                                                 lambda_85[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_14 (Activation)      (2048, 7, 7)         0           lambda_87[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_72 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_28 (Dropout)            (2048, 7, 7)         0           activation_14[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_86 (Lambda)              (2048, 7, 64)        0           dense_72[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_88 (Lambda)              (2048, 7, 64)        0           dropout_28[0][0]                                      \n",
      "                                                                 lambda_86[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_89 (Lambda)              (512, 7, 256)        0           lambda_88[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_14 (TimeDistri (512, 7, 128)        32896       lambda_89[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_29 (Dropout)            (512, 7, 128)        0           time_distributed_14[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_14 (LayerNo (512, 7, 128)        256         dropout_29[0][0]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_29 (Bidirectional (512, 128)           99328       layer_normalization_14[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_29[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            4128        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_74 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,772,962                                                                                                \n",
      "Trainable params: 2,772,194                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.51339287, 0.5223214, 0.5295759, 0.54185265, 0.532087, 0.5379464, 0.547712, 0.546875, 0.54296875, 0.546317, 0.55133927, 0.5499442, 0.5499442, 0.5532924, 0.5552455, 0.5555245, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5549665, 0.55691963, 0.55887276, 0.5605469, 0.5700335, 0.5792411, 0.593192, 0.6021205, 0.60602677, 0.61188614, 0.62109375, 0.610212, 0.6188616, 0.6219308, 0.6152344, 0.62248886, 0.6202567, 0.62220985, 0.625, 0.62081474, 0.62220985, 0.6238839, 0.624163, 0.63727677, 0.63671875, 0.63002235, 0.6269531, 0.6314174, 0.640346, 0.6342076, 0.6375558, 0.63643974, 0.641183, 0.6280692, 0.6314174, 0.6422991, 0.63643974, 0.63727677, 0.6420201, 0.641183, 0.64285713, 0.64397323, 0.6515067, 0.63811386]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4820772, 0.4990809, 0.5136719, 0.51171875, 0.51263785, 0.5103401, 0.5058594, 0.51516545, 0.5068934, 0.51424634, 0.5289522, 0.5153952, 0.5132123, 0.5261949, 0.5202206, 0.5152803, 0.5243566, 0.52136946, 0.52286303, 0.5303309, 0.5335478, 0.5315947, 0.54067093, 0.5337776, 0.54204965, 0.54113054, 0.5418199, 0.5443474, 0.55204505, 0.5591682, 0.5572151, 0.5550322, 0.55641085, 0.559398, 0.56204045, 0.5606618, 0.5672105, 0.5684743, 0.5724954, 0.56893384, 0.56950825, 0.5701976, 0.5737592, 0.5731847, 0.57421875, 0.5753676, 0.5737592, 0.57410383, 0.57421875, 0.5715763, 0.5736443, 0.57421875, 0.5730699, 0.57513785, 0.5746783, 0.5747932, 0.57548255, 0.57387406, 0.57421875, 0.57456344, 0.5721507, 0.57329965, 0.57651657, 0.5772059, 0.5803079, 0.57640165, 0.5778952, 0.5784697, 0.5866268, 0.5883502, 0.5884651, 0.58386946, 0.58731616, 0.58639705, 0.59639245, 0.59294575, 0.59547335, 0.6059283, 0.60202205, 0.6037454, 0.6004136, 0.60018384, 0.5974265, 0.6025965, 0.6090303, 0.6108686, 0.60638785, 0.6078814, 0.60799634, 0.6116728, 0.61328125, 0.6070772, 0.6152344, 0.6183364, 0.6144301, 0.6197151, 0.61362594, 0.621898, 0.6200597, 0.6251149]\n",
      " 75%|██████████████████████████████████▌           | 15/20 [24:06<08:35, 103.09s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481EDF26C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481EDF26C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481EDF26C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002481EDF26C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_15\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_16 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_30 (Bidirectional (512, 7, 256)        2438144     input_16[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_30[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_75 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_76 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_90 (Lambda)              (1536, 7, 64)        0           dense_75[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_91 (Lambda)              (1536, 7, 64)        0           dense_76[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_93 (Lambda)              (1536, 7, 7)         0           lambda_90[0][0]                                       \n",
      "                                                                 lambda_91[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_15 (Activation)      (1536, 7, 7)         0           lambda_93[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_77 (Dense)                (512, 7, 192)        49152       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_30 (Dropout)            (1536, 7, 7)         0           activation_15[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_92 (Lambda)              (1536, 7, 64)        0           dense_77[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_94 (Lambda)              (1536, 7, 64)        0           dropout_30[0][0]                                      \n",
      "                                                                 lambda_92[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_95 (Lambda)              (512, 7, 192)        0           lambda_94[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_15 (TimeDistri (512, 7, 128)        24704       lambda_95[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_31 (Dropout)            (512, 7, 128)        0           time_distributed_15[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_15 (LayerNo (512, 7, 128)        256         dropout_31[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_31 (Bidirectional (512, 256)           264192      layer_normalization_15[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_31[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_79 (Dense)                (512, 2)             514         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,877,314                                                                                                \n",
      "Trainable params: 2,876,290                                                                                            \n",
      "Non-trainable params: 1,024                                                                                            \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.561663, 0.5566406, 0.57477677, 0.608817, 0.6280692, 0.641183, 0.656808, 0.65959823, 0.68415177, 0.67438614, 0.6967076, 0.69084823, 0.68247765, 0.7251674, 0.72293526, 0.7419085, 0.7360491, 0.7173549, 0.7273995, 0.74469864, 0.7441406, 0.74553573, 0.7441406, 0.74441963, 0.7441406, 0.7419085, 0.74553573, 0.749163, 0.749442, 0.749163, 0.7441406, 0.74469864, 0.7511161, 0.7402344, 0.7433036, 0.734933, 0.7427455, 0.7452567, 0.75223213, 0.74637276, 0.7413505, 0.74748886, 0.7486049, 0.7566964, 0.750837, 0.7575335, 0.73967636, 0.765904, 0.7511161, 0.7433036, 0.74748886, 0.7416295, 0.75223213, 0.75502235, 0.74553573, 0.7407924, 0.7558594, 0.76227677, 0.7435826, 0.76283485, 0.75418526, 0.75558037, 0.76813614, 0.7558594, 0.76283485, 0.75446427, 0.7558594, 0.75334823, 0.7589286, 0.7435826, 0.7424665, 0.750558, 0.766183, 0.749442, 0.7561384, 0.74637276, 0.7564174, 0.765625, 0.750558, 0.7480469, 0.7511161, 0.75390625, 0.7564174, 0.7597656, 0.7583705, 0.7519531, 0.76283485, 0.766183, 0.7569755, 0.7575335, 0.7575335, 0.7636719, 0.7608817, 0.7572545, 0.7566964, 0.76339287, 0.7486049, 0.76227677, 0.76032364, 0.76004463]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5175781, 0.5225184, 0.5413603, 0.55261946, 0.57858455, 0.612477, 0.66050094, 0.68267465, 0.72357535, 0.7409237, 0.74264705, 0.7630974, 0.7636719, 0.76953125, 0.7735524, 0.7767693, 0.78067553, 0.7830882, 0.78079045, 0.78504133, 0.78504133, 0.7880285, 0.79526657, 0.79113054, 0.7972197, 0.78860295, 0.79067093, 0.7946921, 0.79078585, 0.8019301, 0.7991728, 0.7921645, 0.79526657, 0.80422795, 0.7953814, 0.79480696, 0.79951745, 0.7925092, 0.793773, 0.8019301, 0.80526197, 0.79733455, 0.801011, 0.7946921, 0.7977941, 0.7957261, 0.79388785, 0.79894304, 0.8018153, 0.79951745, 0.7988281, 0.79641545, 0.7945772, 0.79630053, 0.8013557, 0.7957261, 0.7997472, 0.7990579, 0.79630053, 0.793773, 0.79423255, 0.7946921, 0.7949219, 0.8025046, 0.8049173, 0.80112594, 0.8037684, 0.80112594, 0.8027344, 0.8050322, 0.8012408, 0.7972197, 0.8028493, 0.8075597, 0.80112594, 0.8002068, 0.7996324, 0.8071002, 0.7987132, 0.8065257, 0.80675554, 0.8044577, 0.8034237, 0.80089617, 0.8005515, 0.8122702, 0.80296415, 0.80422795, 0.8097426, 0.8077895, 0.8021599, 0.8034237, 0.8056066, 0.8050322, 0.8061811, 0.8037684, 0.8003217, 0.8019301, 0.8046875, 0.80066633]\n",
      " 80%|████████████████████████████████████▊         | 16/20 [25:57<07:01, 105.46s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000248071A9E88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000248071A9E88>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000248071A9E88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x00000248071A9E88>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_16\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_17 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_32 (Bidirectional (512, 7, 128)        1186304     input_17[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_32[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_80 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_81 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_96 (Lambda)              (1536, 7, 64)        0           dense_80[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_97 (Lambda)              (1536, 7, 64)        0           dense_81[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_99 (Lambda)              (1536, 7, 7)         0           lambda_96[0][0]                                       \n",
      "                                                                 lambda_97[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_16 (Activation)      (1536, 7, 7)         0           lambda_99[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_82 (Dense)                (512, 7, 192)        24576       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_32 (Dropout)            (1536, 7, 7)         0           activation_16[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_98 (Lambda)              (1536, 7, 64)        0           dense_82[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_100 (Lambda)             (1536, 7, 64)        0           dropout_32[0][0]                                      \n",
      "                                                                 lambda_98[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_101 (Lambda)             (512, 7, 192)        0           lambda_100[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_16 (TimeDistri (512, 7, 32)         6176        lambda_101[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_33 (Dropout)            (512, 7, 32)         0           time_distributed_16[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_16 (LayerNo (512, 7, 32)         64          dropout_33[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_33 (Bidirectional (512, 256)           165888      layer_normalization_16[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 256)           1024        bidirectional_33[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 256)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 16)            4112        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 16)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_84 (Dense)                (512, 2)             34          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,437,842                                                                                                \n",
      "Trainable params: 1,437,074                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc of epoch:                                                                                                    \n",
      "[0.5674403, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367, 0.57433367]\n",
      " 85%|███████████████████████████████████████       | 17/20 [27:37<05:11, 103.81s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482CDACEC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482CDACEC8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482CDACEC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482CDACEC8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_17\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_18 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_34 (Bidirectional (512, 7, 128)        1186304     input_18[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_34[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 128)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_85 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_86 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_102 (Lambda)             (2048, 7, 64)        0           dense_85[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_103 (Lambda)             (2048, 7, 64)        0           dense_86[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_105 (Lambda)             (2048, 7, 7)         0           lambda_102[0][0]                                      \n",
      "                                                                 lambda_103[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_17 (Activation)      (2048, 7, 7)         0           lambda_105[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_87 (Dense)                (512, 7, 256)        32768       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_34 (Dropout)            (2048, 7, 7)         0           activation_17[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_104 (Lambda)             (2048, 7, 64)        0           dense_87[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_106 (Lambda)             (2048, 7, 64)        0           dropout_34[0][0]                                      \n",
      "                                                                 lambda_104[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_107 (Lambda)             (512, 7, 256)        0           lambda_106[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_17 (TimeDistri (512, 7, 64)         16448       lambda_107[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_35 (Dropout)            (512, 7, 64)         0           time_distributed_17[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_17 (LayerNo (512, 7, 64)         128         dropout_35[0][0]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_35 (Bidirectional (512, 64)            25088       layer_normalization_17[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 64)            256         bidirectional_35[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 64)            0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_89 (Dense)                (512, 2)             130         x7[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 1,327,170                                                                                                \n",
      "Trainable params: 1,326,786                                                                                            \n",
      "Non-trainable params: 384                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5172991, 0.53404015, 0.546875, 0.5376674, 0.54129463, 0.5485491, 0.547433, 0.547712, 0.5499442, 0.547712, 0.55245537, 0.55747765, 0.55831474, 0.55747765, 0.5566406, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5552455, 0.5546875, 0.5552455, 0.55719864, 0.5546875, 0.5636161, 0.57310265, 0.5636161, 0.5605469, 0.56501114, 0.5625, 0.562779, 0.5613839, 0.5611049, 0.563058, 0.5711495, 0.563058, 0.577288, 0.5705915, 0.5719866, 0.578404, 0.58091515, 0.5864955, 0.594029, 0.594587, 0.5845424, 0.594587, 0.58984375, 0.58816963, 0.58035713, 0.59654015, 0.5848214, 0.5948661, 0.59626114, 0.5839844]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.50620407, 0.50356156, 0.5099954, 0.5144761, 0.5159697, 0.50482535, 0.5097656, 0.5243566, 0.5190717, 0.5294118, 0.5319393, 0.53182447, 0.5446921, 0.53699446, 0.54549634, 0.55870867, 0.5649127, 0.5672105, 0.5688189, 0.5678998, 0.5714614, 0.5718061, 0.5734145, 0.5731847, 0.5721507, 0.57410383, 0.5728401, 0.5735294, 0.5736443, 0.5744485, 0.57421875, 0.5731847, 0.57456344, 0.57387406, 0.573989, 0.57295495, 0.5746783, 0.5736443, 0.573989, 0.5737592, 0.573989, 0.5746783, 0.57421875, 0.57410383, 0.5744485, 0.57329965, 0.57410383, 0.5736443, 0.57433367, 0.57421875, 0.57410383, 0.5744485, 0.57433367, 0.5744485, 0.5746783, 0.57433367, 0.57433367, 0.57421875, 0.57410383, 0.57433367, 0.57433367, 0.57410383, 0.57329965, 0.5726103, 0.5731847, 0.57387406, 0.573989, 0.57421875, 0.57421875, 0.57421875, 0.57387406, 0.57387406, 0.57387406, 0.57410383, 0.5747932, 0.5758272, 0.5722656, 0.5731847, 0.5735294, 0.5727252, 0.5749081, 0.5752528, 0.573989, 0.5722656, 0.57421875, 0.5730699, 0.573989, 0.5701976, 0.5752528, 0.5721507, 0.5753676, 0.57077205, 0.5753676, 0.5757123, 0.56985295, 0.5706572, 0.5761719, 0.5780101, 0.5753676, 0.57651657]\n",
      " 90%|█████████████████████████████████████████▍    | 18/20 [29:19<03:26, 103.31s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EC8F9C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EC8F9C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EC8F9C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EC8F9C8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_18\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_19 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_36 (Bidirectional (512, 7, 256)        2438144     input_19[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_36[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_90 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_91 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_108 (Lambda)             (2048, 7, 64)        0           dense_90[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_109 (Lambda)             (2048, 7, 64)        0           dense_91[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_111 (Lambda)             (2048, 7, 7)         0           lambda_108[0][0]                                      \n",
      "                                                                 lambda_109[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_18 (Activation)      (2048, 7, 7)         0           lambda_111[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_92 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_36 (Dropout)            (2048, 7, 7)         0           activation_18[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_110 (Lambda)             (2048, 7, 64)        0           dense_92[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_112 (Lambda)             (2048, 7, 64)        0           dropout_36[0][0]                                      \n",
      "                                                                 lambda_110[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_113 (Lambda)             (512, 7, 256)        0           lambda_112[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_18 (TimeDistri (512, 7, 128)        32896       lambda_113[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_37 (Dropout)            (512, 7, 128)        0           time_distributed_18[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_18 (LayerNo (512, 7, 128)        256         dropout_37[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_37 (Bidirectional (512, 128)           99328       layer_normalization_18[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 128)           512         bidirectional_37[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 128)           0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            4128        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_94 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,772,962                                                                                                \n",
      "Trainable params: 2,772,194                                                                                            \n",
      "Non-trainable params: 768                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5301339, 0.51981026, 0.52762276, 0.5295759, 0.5421317, 0.530692, 0.5541295, 0.5608259, 0.56026787, 0.5694755, 0.56696427, 0.5714286, 0.5792411, 0.5876116, 0.59375, 0.6082589, 0.59765625, 0.62053573, 0.6185826, 0.6336495, 0.63783485, 0.640067, 0.63532364, 0.6395089, 0.63783485, 0.64313614, 0.64453125, 0.6481585, 0.6476005, 0.65848213, 0.66015625, 0.66043526, 0.6623884, 0.6582031, 0.65876114, 0.656529, 0.66852677, 0.6699219, 0.6651786, 0.6629464, 0.66043526, 0.6729911, 0.671038, 0.671038, 0.671596, 0.67522323, 0.67633927, 0.66741073, 0.67606026, 0.66685265, 0.67466515, 0.672154, 0.66852677, 0.6780134, 0.6749442, 0.68359375, 0.6799665, 0.6794085, 0.672712, 0.6796875, 0.68470985, 0.6863839, 0.6749442, 0.6732701, 0.6891741, 0.67745537, 0.6660156, 0.67438614, 0.6844308, 0.6808036, 0.6749442, 0.67410713, 0.671038, 0.68387276, 0.69112724, 0.6766183, 0.6905692, 0.6863839, 0.67745537, 0.68498886, 0.69893974, 0.69029015, 0.67550224, 0.67633927, 0.68387276, 0.6813616, 0.68191963, 0.6827567, 0.6875, 0.688058, 0.6861049, 0.68359375, 0.6816406, 0.68359375, 0.6749442, 0.6688058, 0.67578125, 0.68526787, 0.67522323, 0.6796875]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5335478, 0.52297795, 0.5230928, 0.53182447, 0.5266544, 0.53331804, 0.5465303, 0.5443474, 0.53986675, 0.5543428, 0.55089617, 0.5546875, 0.5650276, 0.5650276, 0.5759421, 0.5736443, 0.57755053, 0.5919118, 0.5983456, 0.59777117, 0.61144304, 0.620864, 0.62293196, 0.61776197, 0.629136, 0.6294807, 0.6415441, 0.64326745, 0.64418656, 0.6564798, 0.65360755, 0.65222883, 0.65326285, 0.6563649, 0.6573989, 0.6533778, 0.66107535, 0.65923715, 0.6596967, 0.6640625, 0.66463697, 0.66004133, 0.6625689, 0.6706112, 0.6639476, 0.66567093, 0.670841, 0.6637178, 0.67589617, 0.6715303, 0.67015165, 0.66796875, 0.6695772, 0.67704505, 0.6616498, 0.6777344, 0.6703814, 0.6700368, 0.66544116, 0.6711857, 0.6709559, 0.6717601, 0.6738281, 0.6644072, 0.6784237, 0.6685432, 0.66980696, 0.6725643, 0.6785386, 0.67141545, 0.6716452, 0.6746324, 0.6738281, 0.6803768, 0.67486215, 0.67578125, 0.68106616, 0.6753217, 0.6709559, 0.6763557, 0.66923255, 0.6793428, 0.676011, 0.67543656, 0.6722197, 0.6787684, 0.68267465, 0.6796875, 0.6798024, 0.676011, 0.6771599, 0.6764706, 0.6807215, 0.67578125, 0.6725643, 0.6737132, 0.6808364, 0.6807215, 0.683364, 0.6765855]\n",
      " 95%|███████████████████████████████████████████▋  | 19/20 [31:15<01:47, 107.18s/trial, best loss: -0.7804129719734192]WARNING:tensorflow:Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EF05688>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EF05688>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EF05688>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method keras_fmin_fnct.<locals>.LayerNormalization.call of <temp_model.keras_fmin_fnct.<locals>.LayerNormalization object at 0x000002482EF05688>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "Model: \"model_19\"                                                                                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_20 (InputLayer)           [(512, 7, 2251)]     0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_38 (Bidirectional (512, 7, 256)        2438144     input_20[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (BatchNormalization)         (512, 7, 256)        1024        bidirectional_38[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Dropout)                    (512, 7, 256)        0           x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_95 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_96 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_114 (Lambda)             (2048, 7, 64)        0           dense_95[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_115 (Lambda)             (2048, 7, 64)        0           dense_96[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_117 (Lambda)             (2048, 7, 7)         0           lambda_114[0][0]                                      \n",
      "                                                                 lambda_115[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_19 (Activation)      (2048, 7, 7)         0           lambda_117[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_97 (Dense)                (512, 7, 256)        65536       x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_38 (Dropout)            (2048, 7, 7)         0           activation_19[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_116 (Lambda)             (2048, 7, 64)        0           dense_97[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_118 (Lambda)             (2048, 7, 64)        0           dropout_38[0][0]                                      \n",
      "                                                                 lambda_116[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_119 (Lambda)             (512, 7, 256)        0           lambda_118[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_19 (TimeDistri (512, 7, 32)         8224        lambda_119[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_39 (Dropout)            (512, 7, 32)         0           time_distributed_19[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_19 (LayerNo (512, 7, 32)         64          dropout_39[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_39 (Bidirectional (512, 64)            16896       layer_normalization_19[0][0]                          \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (BatchNormalization)         (512, 64)            256         bidirectional_39[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x7 (Dropout)                    (512, 64)            0           x6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x8 (Dense)                      (512, 32)            2080        x7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x9 (Dropout)                    (512, 32)            0           x8[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_99 (Dense)                (512, 2)             66          x9[0][0]                                              \n",
      "==================================================================================================                     \n",
      "Total params: 2,663,362                                                                                                \n",
      "Trainable params: 2,662,722                                                                                            \n",
      "Non-trainable params: 640                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.547712, 0.561942, 0.56668526, 0.5560826, 0.5638951, 0.5613839, 0.561942, 0.55859375, 0.55831474, 0.561663, 0.5611049, 0.56473213, 0.55998886, 0.5641741, 0.56584823, 0.55998886, 0.5613839, 0.58203125, 0.6018415, 0.59793526, 0.625837, 0.625837, 0.64313614, 0.6481585, 0.6629464, 0.66824776, 0.6735491, 0.6643415, 0.6738281, 0.6780134, 0.6799665, 0.6827567, 0.6816406, 0.6782924, 0.68973213, 0.68191963, 0.6875, 0.6802455, 0.68498886, 0.6941964, 0.6961495, 0.6894531, 0.69280136, 0.6950335, 0.6944755, 0.6891741, 0.7117745, 0.6967076, 0.6961495, 0.68470985, 0.6855469, 0.68973213, 0.6950335, 0.69921875, 0.69112724, 0.69168526, 0.6944755, 0.69196427, 0.703404, 0.6967076, 0.7020089, 0.7000558, 0.6961495, 0.6961495, 0.7000558, 0.7017299, 0.702567, 0.69893974, 0.70563614, 0.6891741, 0.687779, 0.7112165, 0.7000558, 0.6939174, 0.7045201, 0.7047991, 0.7061942, 0.7078683, 0.69949776, 0.70563614, 0.7011719, 0.7045201, 0.7112165, 0.7109375, 0.703404, 0.7017299, 0.69921875, 0.69029015, 0.70535713, 0.70703125, 0.70758927, 0.69810265, 0.7047991, 0.70675224, 0.7050781, 0.71651787, 0.70703125, 0.70647323, 0.70563614, 0.7112165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc of epoch:                                                                                                    \n",
      "[0.5132123, 0.51953125, 0.5127528, 0.5189568, 0.52079505, 0.52952665, 0.5297564, 0.5279182, 0.53550094, 0.5383732, 0.55204505, 0.5465303, 0.54423255, 0.55388325, 0.5543428, 0.5565257, 0.56732535, 0.5844439, 0.5938649, 0.5934053, 0.6130515, 0.61638325, 0.6261489, 0.6485524, 0.6506204, 0.65326285, 0.66463697, 0.6665901, 0.6738281, 0.6793428, 0.6902574, 0.6889936, 0.6925551, 0.6907169, 0.6894531, 0.6919807, 0.6931296, 0.6972656, 0.6950827, 0.6931296, 0.69485295, 0.71036303, 0.6984145, 0.7100184, 0.7042739, 0.7024357, 0.69795495, 0.70484835, 0.7068015, 0.6971507, 0.7105928, 0.7034697, 0.7068015, 0.70358455, 0.7089844, 0.7027803, 0.706227, 0.704159, 0.70932907, 0.71300554, 0.7038143, 0.70576745, 0.707261, 0.71254593, 0.70668656, 0.7028952, 0.7080653, 0.71392465, 0.7027803, 0.706227, 0.70450366, 0.706227, 0.70668656, 0.70576745, 0.71300554, 0.71714157, 0.70714617, 0.7054228, 0.7094439, 0.7186351, 0.7024357, 0.7167969, 0.7076057, 0.71300554, 0.7112822, 0.70795035, 0.7108226, 0.70450366, 0.71231616, 0.7108226, 0.7058824, 0.7065717, 0.7105928, 0.71610755, 0.7141544, 0.7164522, 0.71300554, 0.71231616, 0.7247243, 0.7059972]\n",
      "100%|███████████████████████████████████████████████| 20/20 [33:10<00:00, 99.54s/trial, best loss: -0.7804129719734192]\n",
      "Evaluation of best performing model:\n",
      "3584/3584 [==============================] - 0s 56us/sample - loss: 1.3444 - acc: 0.7687\n",
      "test_score:  1.3444400514875139  test_accuracy:  0.7686942\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout': 0.08745906293812011, 'Dropout_1': 0, 'Dropout_2': 0.5335531180278833, 'Dropout_3': 0.1281579174820835, 'Dropout_4': 0.10618158219461116, 'Dropout_5': 0.017861473716917042, 'Dropout_6': 0, 'Dropout_7': 0.17421682426380158, 'units': 1, 'units_1': 2, 'units_2': 2, 'units_3': 1, 'units_4': 1, 'units_5': 1}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x_train, x_test, y_train, y_test, batch_size, _ ,_ = data()\n",
    "    print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "    \n",
    "    best_run, best_model = optim.minimize(model=create_multi_attention_bidirectional_cudnnlstm_model, data=data ,algo=tpe.suggest, max_evals=20,trials=Trials(), notebook_name='5.3_old Multi_Attention_Bidirectional_CuDNNLSTM',rseed=1, verbose=False)\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    #best_model.save(\"MULTI_ATTENTION_BIDIRECTIONAL_CUDNNLSTM_bestmodel.h5\")\n",
    "    #print(best_model.get_config())\n",
    "    test_score, test_accuracy = best_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('test_score: ', test_score, ' test_accuracy: ', test_accuracy)\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='5.3 Multi_Attention_Bidirectional_CuDNNLSTM.h5', save_best_only=True, monitor='val_loss', verbose=0, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = best_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size, epochs=1000, validation_data=(x_test, y_test), verbose=0, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GSPC_7days_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf115]",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

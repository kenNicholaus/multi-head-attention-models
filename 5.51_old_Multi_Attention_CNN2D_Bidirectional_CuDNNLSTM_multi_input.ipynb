{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf you are running locally then \\n1. reboot your local machine\\n2. create an environment called 'colab' using anaconda prompt\\nif you have a gpu\\nconda create -n colab python tensorflow-gpu \\nif not \\nconda create -n colab python tensorflow\\n3. to install jupyter notebook\\nconda install jupyter notebook\\n4. to go to the 'colab' environment\\nactivate colab\\n5. change file path to locate this notebook and then type 'jupyter notebook'\\n\\nIf you use colab\\n1. save the data file in your google drive\\n2. goto colab and start running the code\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are running locally then \n",
    "1. reboot your local machine\n",
    "2. create an environment called 'colab' using anaconda prompt\n",
    "if you have a gpu\n",
    "conda create -n colab python tensorflow-gpu \n",
    "if not \n",
    "conda create -n colab python tensorflow\n",
    "3. to install jupyter notebook\n",
    "conda install jupyter notebook\n",
    "4. to go to the 'colab' environment\n",
    "activate colab\n",
    "5. change file path to locate this notebook and then type 'jupyter notebook'\n",
    "\n",
    "If you use colab\n",
    "1. save the data file in your google drive\n",
    "2. goto colab and start running the code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install prominent libraries with specific versions\n",
    "\n",
    "#!pip install tensorflow==1.15.0\n",
    "#!pip install keras==2.2.4-tf\n",
    "#!pip install pandas==0.25.1\n",
    "#!pip install sklearn==0.21.3\n",
    "#!pip install matplotlib==3.2.1\n",
    "#!pip install hyperas\n",
    "#!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "sJnnN6xG_yaM",
    "outputId": "b6420ce8-7f05-440c-e2ce-479b5bc1cafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kenneth\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, CuDNNLSTM, GRU, Input, Activation, Flatten, BatchNormalization, Reshape,Bidirectional\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, History, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras import regularizers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from tensorflow.python.keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from sklearn.preprocessing import *\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from tensorflow.python.keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\n",
    "from tensorflow.python.keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import initializers, regularizers, constraints\n",
    "from tensorflow.python.keras.initializers import *\n",
    "import hyperas\n",
    "import hyperopt\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.__version__ =  1.10.0\n",
      "sklearn.__version__ =  0.21.3\n",
      "numpy.__version__ =  1.17.0\n",
      "pandas.__version__ =  1.0.1\n",
      "matplotlib.__version__ =  3.1.3\n"
     ]
    }
   ],
   "source": [
    "#Get library versions\n",
    "print(\"tensorflow.__version__ = \", tf.__version__)\n",
    "# import tensorflow.python.keras\n",
    "# print(\"keras.__version__ = \", tensorflow.python.keras.__version__)\n",
    "import sklearn \n",
    "print(\"sklearn.__version__ = \", sklearn.__version__)\n",
    "print(\"numpy.__version__ = \", np.__version__)\n",
    "print(\"pandas.__version__ = \", pd.__version__)\n",
    "import matplotlib\n",
    "print(\"matplotlib.__version__ = \", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed to generate reproduceable results\n",
    "from numpy.random import seed\n",
    "seed(56)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(56)\n",
    "random.seed(56)\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they do not exist\n",
    "def build_path(dirName):\n",
    "    try:\n",
    "        os.makedirs(dirName)    \n",
    "        print(\"Directory \" , dirName ,  \" Created \")\n",
    "    except:\n",
    "        print(\"Directory \" , dirName ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8048586833293538556\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7139449242\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5969084720038831421\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# to check if GPU is getting used locally.....you need to see CPU as well as GPU in the output\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOs6JYN__51O"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "  i = 7 # the label target - number of days to predict from the input date\n",
    "  p = 7 #Number of days for target calculated in the data set\n",
    "  batch_size=512\n",
    "  CLASSES = 2\n",
    "  time_steps = 7\n",
    "  ticker='^GSPC'\n",
    "    \n",
    "  # read data\n",
    "  df=pd.read_csv('../^GSPC_7_days_0_return_dtw.csv', index_col = 0, parse_dates = True)\n",
    "    \n",
    "  #add additional rolling mean data  \n",
    "  rm_window =30\n",
    "  rolling_mean = []\n",
    "  for a in range(2,rm_window+1):\n",
    "    df[ticker+'rm_'+str(a)] = df[ticker].rolling(window=rm_window,center=False).mean()\n",
    "    rolling_mean.append(ticker+'rm_'+str(a))\n",
    "    \n",
    "  # create label\n",
    "  targets=pd.DataFrame([])\n",
    "  for j in range (1, p+1):\n",
    "    targets=targets.append(df[ticker+'_{}d_target'.format(j)])\n",
    "    targets=targets.append(df[ticker+'_{}d'.format(j)])\n",
    "  targets=targets.T\n",
    "  df=df.drop(targets.columns, axis=1)\n",
    "  df=df[rm_window:-i]\n",
    "  targets=targets[rm_window:-i]\n",
    "  y=targets['^GSPC_{}d_target'.format(i)]\n",
    "\n",
    "  #check for NaN and remove\n",
    "  df.isna().mean().sum()\n",
    "  y.isna().mean().sum()\n",
    "  remove_list=[]\n",
    "  for i in df.isnull().any().iteritems():\n",
    "    if i[1] == True:\n",
    "      remove_list.append(i[0])\n",
    "  df=df.drop(remove_list, axis=1)\n",
    "  df.isnull().any().mean()\n",
    " \n",
    "  # add percent change\n",
    "  df=df.pct_change()\n",
    "  df=df.replace([np.inf, -np.inf],np.nan) \n",
    "  df.fillna(0, inplace=True)\n",
    "  df.isnull().any().mean()\n",
    "    \n",
    "  # apply preprocessing \n",
    "  x_scaler=RobustScaler()\n",
    "  x = x_scaler.fit_transform(df)\n",
    "  # x_pred = x_scaler.fit_transform(x_pred)\n",
    "  del df\n",
    "  y=y.values\n",
    "    \n",
    "  # apply time steps\n",
    "  def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "      v = X[i:(i + time_steps)]\n",
    "      Xs.append(v)\n",
    "      ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "  x, y = create_dataset(x, y, time_steps)\n",
    "\n",
    "  # create train and test dataset\n",
    "  x_train, x_test, y_train, y_test=train_test_split(x,y, train_size=0.7, random_state=54)\n",
    "  x_train = x_train.astype('float32')\n",
    "  x_test = x_test.astype('float32')\n",
    "  #y_train = y_train.astype('float32')\n",
    "  #y_test = y_test.astype('float32')\n",
    "  y_train = np_utils.to_categorical(y_train, CLASSES, \n",
    "                                    #dtype='float32'\n",
    "                                   )\n",
    "  y_test = np_utils.to_categorical(y_test, CLASSES, \n",
    "                                   #dtype='float32'\n",
    "                                  )\n",
    "    \n",
    "  # adjustment for batch_size\n",
    "  train_start = x_train.shape[0]%batch_size\n",
    "  test_start = x_test.shape[0]%batch_size\n",
    "  x_train = x_train[train_start:]\n",
    "  y_train = y_train[train_start:]\n",
    "  x_test = x_test[test_start:]\n",
    "  y_test = y_test[test_start:]\n",
    "\n",
    "  #reshape for 2D\n",
    "  x_train1 = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2]) \n",
    "  x_test1 = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])  \n",
    "\n",
    "  embed_size = 60\n",
    "\n",
    "  return x_train, x_test, y_train, y_test, batch_size, time_steps, embed_size, x_train1, x_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc8hDOIbADgQ"
   },
   "outputs": [],
   "source": [
    "def create_multi_attention_cnn2D_bidirectional_cudnnlstm_multi_input_model_1(x_train, x_test, y_train, y_test, batch_size, time_steps, embed_size, x_train1, x_test1):\n",
    "    \n",
    "    class LayerNormalization(Layer):\n",
    "        def __init__(self, eps=1e-6, **kwargs):\n",
    "            self.eps = eps\n",
    "            super(LayerNormalization, self).__init__(**kwargs)\n",
    "        def build(self, input_shape):\n",
    "            self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],initializer=Ones(), trainable=True)\n",
    "            self.beta = self.add_weight(name='beta', shape=input_shape[-1:],initializer=Zeros(), trainable=True)\n",
    "            super(LayerNormalization, self).build(input_shape)\n",
    "        def call(self, x):\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.std(x, axis=-1, keepdims=True)\n",
    "            return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return input_shape\n",
    "\n",
    "    class ScaledDotProductAttention():\n",
    "        def __init__(self, d_model, attn_dropout=0.1):\n",
    "            self.temper = np.sqrt(d_model)\n",
    "            self.dropout = Dropout(attn_dropout)\n",
    "        def __call__(self, q, k, v, mask):\n",
    "            attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "            if mask is not None:\n",
    "                mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "                attn = Add()([attn, mmask])\n",
    "            attn = Activation('softmax')(attn)\n",
    "            attn = self.dropout(attn)\n",
    "            output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "            return output, attn\n",
    "\n",
    "    class MultiHeadAttention():\n",
    "        # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "        def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "            self.mode = mode\n",
    "            self.n_head = n_head\n",
    "            self.d_k = d_k\n",
    "            self.d_v = d_v\n",
    "            self.dropout = dropout\n",
    "            if mode == 0:\n",
    "                self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "                self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "                self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "            elif mode == 1:\n",
    "                self.qs_layers = []\n",
    "                self.ks_layers = []\n",
    "                self.vs_layers = []\n",
    "                for _ in range(n_head):\n",
    "                    self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                    self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                    self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "            self.attention = ScaledDotProductAttention(d_model)\n",
    "            self.layer_norm = LayerNormalization() if use_norm else None\n",
    "            self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "        def __call__(self, q, k, v, mask=None):\n",
    "            d_k, d_v = self.d_k, self.d_v\n",
    "            n_head = self.n_head\n",
    "\n",
    "            if self.mode == 0:\n",
    "                qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "                ks = self.ks_layer(k)\n",
    "                vs = self.vs_layer(v)\n",
    "\n",
    "                def reshape1(x):\n",
    "                    s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                    x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                    x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                    x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                    return x\n",
    "                qs = Lambda(reshape1)(qs)\n",
    "                ks = Lambda(reshape1)(ks)\n",
    "                vs = Lambda(reshape1)(vs)\n",
    "\n",
    "                if mask is not None:\n",
    "                    mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "                head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "\n",
    "                def reshape2(x):\n",
    "                    s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                    x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                    x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                    x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                    return x\n",
    "                head = Lambda(reshape2)(head)\n",
    "            elif self.mode == 1:\n",
    "                heads = []; attns = []\n",
    "                for i in range(n_head):\n",
    "                    qs = self.qs_layers[i](q)   \n",
    "                    ks = self.ks_layers[i](k) \n",
    "                    vs = self.vs_layers[i](v) \n",
    "                    head, attn = self.attention(qs, ks, vs, mask)\n",
    "                    heads.append(head); attns.append(attn)\n",
    "                head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "                attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "            outputs = self.w_o(head)\n",
    "            outputs = Dropout(self.dropout)(outputs)\n",
    "            if not self.layer_norm: return outputs, attn\n",
    "            # outputs = Add()([outputs, q]) # sl: fix\n",
    "            return self.layer_norm(outputs), attn\n",
    "\n",
    "    class PositionwiseFeedForward():\n",
    "        def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "            self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "            self.w_2 = Conv1D(d_hid, 1)\n",
    "            self.layer_norm = LayerNormalization()\n",
    "            self.dropout = Dropout(dropout)\n",
    "        def __call__(self, x):\n",
    "            output = self.w_1(x) \n",
    "            output = self.w_2(output)\n",
    "            output = self.dropout(output)\n",
    "            output = Add()([output, x])\n",
    "            return self.layer_norm(output)\n",
    "\n",
    "    class EncoderLayer():\n",
    "        def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "            self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "            self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "        def __call__(self, enc_input, mask=None):\n",
    "            output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "            output = self.pos_ffn_layer(output)\n",
    "            return output, slf_attn\n",
    "\n",
    "\n",
    "    def GetPosEncodingMatrix(max_len, d_emb):\n",
    "        pos_enc = np.array([\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "            if pos != 0 else np.zeros(d_emb) \n",
    "                for pos in range(max_len)\n",
    "                ])\n",
    "        pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "        pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "        return pos_enc\n",
    "\n",
    "    def GetPadMask(q, k):\n",
    "        ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "        mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "        mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "        return mask\n",
    "\n",
    "    def GetSubMask(s):\n",
    "        len_s = tf.shape(s)[1]\n",
    "        bs = tf.shape(s)[:1]\n",
    "        mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "        return mask\n",
    "\n",
    "    class Transformer():\n",
    "        def __init__(self, len_limit, embedding_matrix, d_model=embed_size, \\\n",
    "                  d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n",
    "                  share_word_emb=False, **kwargs):\n",
    "            self.name = 'Transformer'\n",
    "            self.len_limit = len_limit\n",
    "            self.src_loc_info = False # True # sl: fix later\n",
    "            self.d_model = d_model\n",
    "            self.decode_model = None\n",
    "            d_emb = d_model\n",
    "\n",
    "            pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n",
    "                                weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n",
    "\n",
    "            i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n",
    "\n",
    "            self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n",
    "                                   word_emb=i_word_emb, pos_emb=pos_emb)\n",
    "\n",
    "\n",
    "        def get_pos_seq(self, x):\n",
    "            mask = K.cast(K.not_equal(x, 0), 'int32')\n",
    "            pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n",
    "            return pos * mask\n",
    "\n",
    "        \n",
    "    # define two sets of inputs(it can be the same or different)\n",
    "    inputA = Input(batch_shape=(batch_size, x_train1.shape[1], x_train1.shape[2], x_train1.shape[3]), name='input_1')\n",
    "    inputB = Input(batch_shape=(batch_size, x_train.shape[1], x_train.shape[2]), name='input_2')\n",
    "\n",
    "    # the first branch operates on the first input\n",
    "    x1 = Conv2D(filters = {{choice([32, 64, 128])}},name='x1', kernel_size =2,padding='same', activation='relu')(inputA)\n",
    "    x2 = MaxPooling2D(pool_size=(2,2), strides=(2,2), data_format=\"channels_first\", name='x2')(x1)\n",
    "    x3 = Conv2D(filters ={{choice([16, 32, 64])}}, name='x3', kernel_size =2,padding='same', activation='relu')(x2)\n",
    "    x4 = MaxPooling2D(pool_size=(2,2), strides=(2,2), data_format=\"channels_first\", name='x4')(x3)\n",
    "    x5 = Flatten(name='x5')(x4)\n",
    "    x6 = Dense(units = {{choice([4, 8,16,32])}},name='x6', activation='relu')(x5)\n",
    "    x = Model(inputA, x6, name='x')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    y1 = Bidirectional(CuDNNLSTM(units = {{choice([16,32, 64, 128])}}, name='x1',  kernel_regularizer=regularizers.l2(0.03), \n",
    "              bias_regularizer=regularizers.l1(0.03),\n",
    "              return_sequences=True, stateful = True))(inputB)\n",
    "    y2 = BatchNormalization(name='y2')(y1)\n",
    "    y3 = Dropout({{uniform(0, 1)}},name='y3')(y2)\n",
    "    \n",
    "    \n",
    "    if {{choice(['three', 'four','five'])}} == 'three':\n",
    "        y4, slf_attn = MultiHeadAttention(n_head=3, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(y3, y3, y3)\n",
    "    elif 'four':\n",
    "        y4, slf_attn = MultiHeadAttention(n_head=4, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(y3, y3, y3)\n",
    "    elif 'five':\n",
    "        y4, slf_attn = MultiHeadAttention(n_head=5, d_model={{choice([32, 64, 128])}}, d_k=64, d_v=64, dropout={{uniform(0, 1)}})(y3, y3, y3)   \n",
    "    \n",
    "    y5 = Bidirectional(CuDNNLSTM(units = {{choice([4, 8,16,32])}},name='y5',  kernel_regularizer=regularizers.l2(0.03), \n",
    "              bias_regularizer=regularizers.l1(0.03),\n",
    "              return_sequences=False,stateful = True))(y4)\n",
    "    y6 = BatchNormalization(name='y6')(y5)\n",
    "    y7 = Dropout({{uniform(0, 1)}},name='y7')(y6)\n",
    "        \n",
    "    y = Model(inputB, y7, name='y')\n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    combined = Concatenate(name='x_y_output_combined')([x.output, y.output])\n",
    "\n",
    "\n",
    "    # combined outputs\n",
    "    z = Dense(2, name='z', activation=\"softmax\")(combined)\n",
    "\n",
    "    # our model will accept the inputs of the two branches and then output buy or sell\n",
    "    model = Model(inputs=[inputA, inputB], outputs=z)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer = Adam(lr=0.001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    result = model.fit([x_train1, x_train],y_train,batch_size=batch_size,epochs=100,validation_data=([x_test1, x_test], y_test), verbose=0)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "\n",
    "    print('Best validation acc of epoch:', result.history['val_acc'])\n",
    "    print('Train acc of epoch:', result.history['acc'])\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HcxtusN4AK8y",
    "outputId": "43d151c2-cf3b-4ab8-e95f-7c48cef1da49",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8704, 7, 2782) (8704, 2) (3584, 7, 2782) (3584, 2)\n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            (512, 7, 2782)       0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional (Bidirectional)   (512, 7, 32)         358400      input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "y2 (BatchNormalization)         (512, 7, 32)         128         bidirectional[0][0]                                   \n",
      "__________________________________________________________________________________________________                     \n",
      "y3 (Dropout)                    (512, 7, 32)         0           y2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense (Dense)                   (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_1 (Dense)                 (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda (Lambda)                 (2048, 7, 64)        0           dense[0][0]                                           \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_1 (Lambda)               (2048, 7, 64)        0           dense_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_3 (Lambda)               (2048, 7, 7)         0           lambda[0][0]                                          \n",
      "                                                                 lambda_1[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "activation (Activation)         (2048, 7, 7)         0           lambda_3[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_2 (Dense)                 (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout (Dropout)               (2048, 7, 7)         0           activation[0][0]                                      \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_2 (Lambda)               (2048, 7, 64)        0           dense_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_4 (Lambda)               (2048, 7, 64)        0           dropout[0][0]                                         \n",
      "                                                                 lambda_2[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "input_1 (InputLayer)            (512, 1, 7, 2782)    0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_5 (Lambda)               (512, 7, 256)        0           lambda_4[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x1 (Conv2D)                     (512, 1, 7, 32)      356128      input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed (TimeDistribut (512, 7, 128)        32896       lambda_5[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (MaxPooling2D)               (512, 1, 3, 16)      0           x1[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_1 (Dropout)             (512, 7, 128)        0           time_distributed[0][0]                                \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Conv2D)                     (512, 1, 3, 64)      4160        x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization (LayerNorma (512, 7, 128)        256         dropout_1[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x4 (MaxPooling2D)               (512, 1, 1, 32)      0           x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_1 (Bidirectional) (512, 8)             4288        layer_normalization[0][0]                             \n",
      "__________________________________________________________________________________________________                     \n",
      "x5 (Flatten)                    (512, 32)            0           x4[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y6 (BatchNormalization)         (512, 8)             32          bidirectional_1[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (Dense)                      (512, 4)             132         x5[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y7 (Dropout)                    (512, 8)             0           y6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x_y_output_combined (Concatenat (512, 12)            0           x6[0][0]                                              \n",
      "                                                                 y7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "z (Dense)                       (512, 2)             26          x_y_output_combined[0][0]                             \n",
      "==================================================================================================                     \n",
      "Total params: 781,022                                                                                                  \n",
      "Trainable params: 780,942                                                                                              \n",
      "Non-trainable params: 80                                                                                               \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.51171875, 0.5479910714285714, 0.5694754464285714, 0.5767299107142857, 0.58203125, 0.5892857142857143, 0.5934709821428571, 0.6110491071428571, 0.6143973214285714, 0.6057477678571429, 0.6029575892857143, 0.6183035714285714, 0.6420200892857143, 0.6473214285714286, 0.6548549107142857, 0.6554129464285714, 0.6668526785714286, 0.6554129464285714, 0.6648995535714286, 0.66796875, 0.6768973214285714, 0.671875, 0.6810825892857143, 0.6902901785714286, 0.6833147321428571, 0.6802455357142857, 0.6830357142857143, 0.6810825892857143, 0.6869419642857143, 0.6967075892857143, 0.6925223214285714, 0.6844308035714286, 0.6939174107142857, 0.6732700892857143, 0.6763392857142857, 0.5876116071428571, 0.59765625, 0.6623883928571429, 0.6953125, 0.6944754464285714, 0.708984375, 0.7020089285714286, 0.7042410714285714, 0.7022879464285714, 0.6953125, 0.7006138392857143, 0.6975446428571429, 0.7056361607142857, 0.6958705357142857, 0.6939174107142857, 0.6989397321428571, 0.7003348214285714, 0.7061941964285714, 0.701171875, 0.7000558035714286, 0.70703125, 0.7036830357142857, 0.7095424107142857, 0.7039620535714286, 0.7145647321428571, 0.7045200892857143, 0.7092633928571429, 0.716796875, 0.7117745535714286, 0.7073102678571429, 0.71875, 0.6994977678571429, 0.7223772321428571, 0.7156808035714286, 0.7181919642857143, 0.7114955357142857, 0.7114955357142857, 0.7092633928571429, 0.7042410714285714, 0.7109375, 0.7014508928571429, 0.6930803571428571, 0.6877790178571429, 0.7006138392857143, 0.6897321428571429, 0.6981026785714286, 0.7103794642857143, 0.7022879464285714, 0.7176339285714286, 0.7025669642857143, 0.7140066964285714, 0.7134486607142857, 0.71875, 0.7198660714285714, 0.72265625, 0.7179129464285714, 0.7201450892857143, 0.71484375, 0.7087053571428571, 0.7134486607142857, 0.7114955357142857, 0.708984375, 0.7028459821428571, 0.70703125, 0.7056361607142857]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5299862132352942, 0.5096507352941176, 0.5188419117647058, 0.54296875, 0.5454963235294118, 0.5597426470588235, 0.5718060661764706, 0.5849034926470589, 0.6047794117647058, 0.6209788602941176, 0.6236213235294118, 0.6351102941176471, 0.6590073529411765, 0.6678538602941176, 0.6765854779411765, 0.6801470588235294, 0.6915211397058824, 0.6963465073529411, 0.6926700367647058, 0.7026654411764706, 0.7025505514705882, 0.7049632352941176, 0.69921875, 0.7209329044117647, 0.7149586397058824, 0.7223115808823529, 0.7165670955882353, 0.7153033088235294, 0.7118566176470589, 0.7256433823529411, 0.7269071691176471, 0.7284007352941176, 0.73046875, 0.7232306985294118, 0.7202435661764706, 0.6555606617647058, 0.6174172794117647, 0.6631433823529411, 0.7158777573529411, 0.7259880514705882, 0.7329963235294118, 0.7403492647058824, 0.7470128676470589, 0.7426470588235294, 0.7459788602941176, 0.7472426470588235, 0.7459788602941176, 0.7372472426470589, 0.7414981617647058, 0.7497702205882353, 0.7464384191176471, 0.744140625, 0.7425321691176471, 0.7491957720588235, 0.7446001838235294, 0.7462086397058824, 0.7433363970588235, 0.7471277573529411, 0.7514935661764706, 0.7491957720588235, 0.7443704044117647, 0.7564338235294118, 0.7525275735294118, 0.7479319852941176, 0.7562040441176471, 0.7521829044117647, 0.736328125, 0.7433363970588235, 0.7493106617647058, 0.7388556985294118, 0.7528722426470589, 0.7534466911764706, 0.7514935661764706, 0.7488511029411765, 0.7424172794117647, 0.7463235294117647, 0.7440257352941176, 0.7464384191176471, 0.740234375, 0.7433363970588235, 0.7427619485294118, 0.7378216911764706, 0.7463235294117647, 0.7508042279411765, 0.7552849264705882, 0.7536764705882353, 0.7510340073529411, 0.7505744485294118, 0.7470128676470589, 0.7450597426470589, 0.7444852941176471, 0.748046875, 0.7424172794117647, 0.7467830882352942, 0.7428768382352942, 0.7431066176470589, 0.7456341911764706, 0.7364430147058824, 0.7373621323529411, 0.7470128676470589]\n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            (512, 7, 2782)       0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_2 (Bidirectional) (512, 7, 32)         358400      input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "y2 (BatchNormalization)         (512, 7, 32)         128         bidirectional_2[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "y3 (Dropout)                    (512, 7, 32)         0           y2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_4 (Dense)                 (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_5 (Dense)                 (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_6 (Lambda)               (2048, 7, 64)        0           dense_4[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_7 (Lambda)               (2048, 7, 64)        0           dense_5[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_9 (Lambda)               (2048, 7, 7)         0           lambda_6[0][0]                                        \n",
      "                                                                 lambda_7[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_1 (Activation)       (2048, 7, 7)         0           lambda_9[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_6 (Dense)                 (512, 7, 256)        8192        y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_2 (Dropout)             (2048, 7, 7)         0           activation_1[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_8 (Lambda)               (2048, 7, 64)        0           dense_6[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_10 (Lambda)              (2048, 7, 64)        0           dropout_2[0][0]                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 lambda_8[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "input_1 (InputLayer)            (512, 1, 7, 2782)    0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_11 (Lambda)              (512, 7, 256)        0           lambda_10[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x1 (Conv2D)                     (512, 1, 7, 32)      356128      input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_1 (TimeDistrib (512, 7, 64)         16448       lambda_11[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (MaxPooling2D)               (512, 1, 3, 16)      0           x1[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_3 (Dropout)             (512, 7, 64)         0           time_distributed_1[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Conv2D)                     (512, 1, 3, 64)      4160        x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_1 (LayerNor (512, 7, 64)         128         dropout_3[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x4 (MaxPooling2D)               (512, 1, 1, 32)      0           x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_3 (Bidirectional) (512, 64)            25088       layer_normalization_1[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x5 (Flatten)                    (512, 32)            0           x4[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y6 (BatchNormalization)         (512, 64)            256         bidirectional_3[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (Dense)                      (512, 8)             264         x5[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y7 (Dropout)                    (512, 64)            0           y6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x_y_output_combined (Concatenat (512, 72)            0           x6[0][0]                                              \n",
      "                                                                 y7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "z (Dense)                       (512, 2)             146         x_y_output_combined[0][0]                             \n",
      "==================================================================================================                     \n",
      "Total params: 785,722                                                                                                  \n",
      "Trainable params: 785,530                                                                                              \n",
      "Non-trainable params: 192                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5290178571428571, 0.5432477678571429, 0.5438058035714286, 0.52734375, 0.5354352678571429, 0.5446428571428571, 0.544921875, 0.544921875, 0.5443638392857143, 0.5401785714285714, 0.5376674107142857, 0.5306919642857143, 0.5256696428571429, 0.5217633928571429, 0.5239955357142857, 0.5326450892857143, 0.5337611607142857, 0.5323660714285714, 0.5306919642857143, 0.5237165178571429, 0.5172991071428571, 0.5237165178571429, 0.5256696428571429, 0.5368303571428571, 0.5376674107142857, 0.5329241071428571, 0.5220424107142857, 0.5128348214285714, 0.486328125, 0.48660714285714285, 0.4974888392857143, 0.501953125, 0.4955357142857143, 0.4868861607142857, 0.4838169642857143, 0.4868861607142857, 0.486328125, 0.5212053571428571, 0.5217633928571429, 0.5223214285714286, 0.5206473214285714, 0.5237165178571429, 0.5164620535714286, 0.5139508928571429, 0.5147879464285714, 0.5159040178571429, 0.5181361607142857, 0.517578125, 0.5270647321428571, 0.5318080357142857, 0.5315290178571429, 0.5320870535714286, 0.5315290178571429, 0.5315290178571429, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5315290178571429, 0.5318080357142857, 0.5318080357142857, 0.5315290178571429, 0.5318080357142857, 0.5315290178571429, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5315290178571429, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.5318080357142857, 0.53125, 0.53125, 0.53125, 0.53125, 0.5315290178571429, 0.53125, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5315290178571429, 0.5318080357142857, 0.53125, 0.53125, 0.53125, 0.5318080357142857, 0.5315290178571429, 0.5318080357142857, 0.5318080357142857, 0.5315290178571429, 0.5315290178571429]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.4788602941176471, 0.5496323529411765, 0.5667509191176471, 0.5602022058823529, 0.556640625, 0.5603170955882353, 0.5615808823529411, 0.5646829044117647, 0.5692784926470589, 0.5710018382352942, 0.5693933823529411, 0.5630744485294118, 0.5581341911764706, 0.5558363970588235, 0.5482536764705882, 0.5556066176470589, 0.5660615808823529, 0.564453125, 0.564453125, 0.5657169117647058, 0.5608915441176471, 0.5515854779411765, 0.5595128676470589, 0.5652573529411765, 0.5620404411764706, 0.5623851102941176, 0.560546875, 0.5593979779411765, 0.5193014705882353, 0.5102251838235294, 0.5170036764705882, 0.5220588235294118, 0.5190716911764706, 0.5012637867647058, 0.4983915441176471, 0.49885110294117646, 0.5005744485294118, 0.5085018382352942, 0.5525045955882353, 0.5571001838235294, 0.5569852941176471, 0.5584788602941176, 0.5580193014705882, 0.5467601102941176, 0.546875, 0.55078125, 0.5498621323529411, 0.5506663602941176, 0.5513556985294118, 0.55859375, 0.5592830882352942, 0.5583639705882353, 0.5593979779411765, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5600873161764706, 0.5600873161764706, 0.5600873161764706, 0.5600873161764706, 0.5600873161764706, 0.5600873161764706, 0.5599724264705882, 0.5599724264705882, 0.5599724264705882, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058, 0.5598575367647058]\n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            (512, 7, 2782)       0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_4 (Bidirectional) (512, 7, 64)         720896      input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "y2 (BatchNormalization)         (512, 7, 64)         256         bidirectional_4[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "y3 (Dropout)                    (512, 7, 64)         0           y2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_8 (Dense)                 (512, 7, 256)        16384       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_9 (Dense)                 (512, 7, 256)        16384       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_12 (Lambda)              (2048, 7, 64)        0           dense_8[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_13 (Lambda)              (2048, 7, 64)        0           dense_9[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_15 (Lambda)              (2048, 7, 7)         0           lambda_12[0][0]                                       \n",
      "                                                                 lambda_13[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_2 (Activation)       (2048, 7, 7)         0           lambda_15[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_10 (Dense)                (512, 7, 256)        16384       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_4 (Dropout)             (2048, 7, 7)         0           activation_2[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_14 (Lambda)              (2048, 7, 64)        0           dense_10[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_16 (Lambda)              (2048, 7, 64)        0           dropout_4[0][0]                                       \n",
      "                                                                 lambda_14[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "input_1 (InputLayer)            (512, 1, 7, 2782)    0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_17 (Lambda)              (512, 7, 256)        0           lambda_16[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x1 (Conv2D)                     (512, 1, 7, 64)      712256      input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_2 (TimeDistrib (512, 7, 128)        32896       lambda_17[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (MaxPooling2D)               (512, 1, 3, 32)      0           x1[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_5 (Dropout)             (512, 7, 128)        0           time_distributed_2[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Conv2D)                     (512, 1, 3, 32)      4128        x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_2 (LayerNor (512, 7, 128)        256         dropout_5[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x4 (MaxPooling2D)               (512, 1, 1, 16)      0           x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_5 (Bidirectional) (512, 32)            18688       layer_normalization_2[0][0]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________                     \n",
      "x5 (Flatten)                    (512, 16)            0           x4[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y6 (BatchNormalization)         (512, 32)            128         bidirectional_5[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (Dense)                      (512, 32)            544         x5[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y7 (Dropout)                    (512, 32)            0           y6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x_y_output_combined (Concatenat (512, 64)            0           x6[0][0]                                              \n",
      "                                                                 y7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "z (Dense)                       (512, 2)             130         x_y_output_combined[0][0]                             \n",
      "==================================================================================================                     \n",
      "Total params: 1,539,330                                                                                                \n",
      "Trainable params: 1,539,138                                                                                            \n",
      "Non-trainable params: 192                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.5284598214285714, 0.5382254464285714, 0.51953125, 0.49581473214285715, 0.46763392857142855, 0.46763392857142855, 0.46205357142857145, 0.46205357142857145, 0.46261160714285715, 0.46205357142857145, 0.462890625, 0.46261160714285715, 0.46316964285714285, 0.462890625, 0.462890625, 0.4634486607142857, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.46316964285714285, 0.4634486607142857, 0.4634486607142857, 0.4634486607142857, 0.4634486607142857, 0.4634486607142857, 0.4634486607142857, 0.4634486607142857, 0.46372767857142855, 0.46372767857142855, 0.46372767857142855, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.46400669642857145, 0.5421316964285714, 0.5482700892857143, 0.546875, 0.55078125, 0.5538504464285714, 0.556640625, 0.5569196428571429, 0.5588727678571429, 0.5608258928571429, 0.5599888392857143, 0.5583147321428571, 0.55859375, 0.55859375, 0.5580357142857143, 0.5518973214285714, 0.5530133928571429, 0.5513392857142857, 0.5524553571428571, 0.5530133928571429, 0.5532924107142857, 0.5524553571428571, 0.5524553571428571, 0.552734375, 0.552734375, 0.552734375, 0.552734375, 0.5524553571428571, 0.5524553571428571, 0.5524553571428571, 0.5524553571428571, 0.5524553571428571]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.5411305147058824, 0.5390625, 0.5485983455882353, 0.52734375, 0.47208180147058826, 0.4534696691176471, 0.45760569852941174, 0.458984375, 0.45840992647058826, 0.45760569852941174, 0.45760569852941174, 0.4573759191176471, 0.45749080882352944, 0.45749080882352944, 0.4577205882352941, 0.4579503676470588, 0.4580652573529412, 0.4579503676470588, 0.45783547794117646, 0.45783547794117646, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4580652573529412, 0.45818014705882354, 0.4580652573529412, 0.45818014705882354, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.45818014705882354, 0.4579503676470588, 0.4579503676470588, 0.4579503676470588, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.4580652573529412, 0.45840992647058826, 0.45840992647058826, 0.45840992647058826, 0.45840992647058826, 0.45840992647058826, 0.4582950367647059, 0.45840992647058826, 0.45840992647058826, 0.45840992647058826, 0.45852481617647056, 0.5471047794117647, 0.5614659926470589, 0.5598575367647058, 0.5558363970588235, 0.5621553308823529, 0.5628446691176471, 0.5693933823529411, 0.5693933823529411, 0.5692784926470589, 0.5728400735294118, 0.5720358455882353, 0.5700827205882353, 0.5697380514705882, 0.5698529411764706, 0.5690487132352942, 0.5650275735294118, 0.5674402573529411, 0.5743336397058824, 0.5769761029411765, 0.5780101102941176, 0.5774356617647058, 0.5770909926470589, 0.5772058823529411, 0.5772058823529411, 0.5770909926470589, 0.5770909926470589, 0.5770909926470589, 0.5770909926470589, 0.5770909926470589, 0.5770909926470589, 0.5770909926470589]\n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            (512, 7, 2782)       0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_6 (Bidirectional) (512, 7, 64)         720896      input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "y2 (BatchNormalization)         (512, 7, 64)         256         bidirectional_6[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "y3 (Dropout)                    (512, 7, 64)         0           y2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_12 (Dense)                (512, 7, 192)        12288       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_13 (Dense)                (512, 7, 192)        12288       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_18 (Lambda)              (1536, 7, 64)        0           dense_12[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_19 (Lambda)              (1536, 7, 64)        0           dense_13[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_21 (Lambda)              (1536, 7, 7)         0           lambda_18[0][0]                                       \n",
      "                                                                 lambda_19[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_3 (Activation)       (1536, 7, 7)         0           lambda_21[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_14 (Dense)                (512, 7, 192)        12288       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_6 (Dropout)             (1536, 7, 7)         0           activation_3[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_20 (Lambda)              (1536, 7, 64)        0           dense_14[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_22 (Lambda)              (1536, 7, 64)        0           dropout_6[0][0]                                       \n",
      "                                                                 lambda_20[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "input_1 (InputLayer)            (512, 1, 7, 2782)    0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_23 (Lambda)              (512, 7, 192)        0           lambda_22[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x1 (Conv2D)                     (512, 1, 7, 64)      712256      input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_3 (TimeDistrib (512, 7, 128)        24704       lambda_23[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (MaxPooling2D)               (512, 1, 3, 32)      0           x1[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_7 (Dropout)             (512, 7, 128)        0           time_distributed_3[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Conv2D)                     (512, 1, 3, 32)      4128        x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_3 (LayerNor (512, 7, 128)        256         dropout_7[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x4 (MaxPooling2D)               (512, 1, 1, 16)      0           x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_7 (Bidirectional) (512, 32)            18688       layer_normalization_3[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x5 (Flatten)                    (512, 16)            0           x4[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y6 (BatchNormalization)         (512, 32)            128         bidirectional_7[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (Dense)                      (512, 32)            544         x5[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y7 (Dropout)                    (512, 32)            0           y6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x_y_output_combined (Concatenat (512, 64)            0           x6[0][0]                                              \n",
      "                                                                 y7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "z (Dense)                       (512, 2)             130         x_y_output_combined[0][0]                             \n",
      "==================================================================================================                     \n",
      "Total params: 1,518,850                                                                                                \n",
      "Trainable params: 1,518,658                                                                                            \n",
      "Non-trainable params: 192                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143, 0.4447544642857143]\n",
      "Train acc of epoch:                                                                                                    \n",
      "[0.43278952205882354, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764, 0.42566636029411764]\n",
      "__________________________________________________________________________________________________                     \n",
      "Layer (type)                    Output Shape         Param #     Connected to                                          \n",
      "==================================================================================================                     \n",
      "input_2 (InputLayer)            (512, 7, 2782)       0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_8 (Bidirectional) (512, 7, 128)        1458176     input_2[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "y2 (BatchNormalization)         (512, 7, 128)        512         bidirectional_8[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "y3 (Dropout)                    (512, 7, 128)        0           y2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_16 (Dense)                (512, 7, 192)        24576       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_17 (Dense)                (512, 7, 192)        24576       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_24 (Lambda)              (1536, 7, 64)        0           dense_16[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_25 (Lambda)              (1536, 7, 64)        0           dense_17[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_27 (Lambda)              (1536, 7, 7)         0           lambda_24[0][0]                                       \n",
      "                                                                 lambda_25[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "activation_4 (Activation)       (1536, 7, 7)         0           lambda_27[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "dense_18 (Dense)                (512, 7, 192)        24576       y3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_8 (Dropout)             (1536, 7, 7)         0           activation_4[0][0]                                    \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_26 (Lambda)              (1536, 7, 64)        0           dense_18[0][0]                                        \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_28 (Lambda)              (1536, 7, 64)        0           dropout_8[0][0]                                       \n",
      "                                                                 lambda_26[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "input_1 (InputLayer)            (512, 1, 7, 2782)    0                                                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "lambda_29 (Lambda)              (512, 7, 192)        0           lambda_28[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x1 (Conv2D)                     (512, 1, 7, 32)      356128      input_1[0][0]                                         \n",
      "__________________________________________________________________________________________________                     \n",
      "time_distributed_4 (TimeDistrib (512, 7, 128)        24704       lambda_29[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x2 (MaxPooling2D)               (512, 1, 3, 16)      0           x1[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "dropout_9 (Dropout)             (512, 7, 128)        0           time_distributed_4[0][0]                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x3 (Conv2D)                     (512, 1, 3, 16)      1040        x2[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "layer_normalization_4 (LayerNor (512, 7, 128)        256         dropout_9[0][0]                                       \n",
      "__________________________________________________________________________________________________                     \n",
      "x4 (MaxPooling2D)               (512, 1, 1, 8)       0           x3[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "bidirectional_9 (Bidirectional) (512, 32)            18688       layer_normalization_4[0][0]                           \n",
      "__________________________________________________________________________________________________                     \n",
      "x5 (Flatten)                    (512, 8)             0           x4[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y6 (BatchNormalization)         (512, 32)            128         bidirectional_9[0][0]                                 \n",
      "__________________________________________________________________________________________________                     \n",
      "x6 (Dense)                      (512, 16)            144         x5[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "y7 (Dropout)                    (512, 32)            0           y6[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "x_y_output_combined (Concatenat (512, 48)            0           x6[0][0]                                              \n",
      "                                                                 y7[0][0]                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "z (Dense)                       (512, 2)             98          x_y_output_combined[0][0]                             \n",
      "==================================================================================================                     \n",
      "Total params: 1,933,602                                                                                                \n",
      "Trainable params: 1,933,282                                                                                            \n",
      "Non-trainable params: 320                                                                                              \n",
      "__________________________________________________________________________________________________                     \n",
      "Best validation acc of epoch:                                                                                          \n",
      "[0.53125, 0.541015625, 0.54296875, 0.5326450892857143, 0.5365513392857143, 0.5387834821428571, 0.5337611607142857, 0.5432477678571429, 0.54296875, 0.5426897321428571, 0.5424107142857143, 0.5435267857142857, 0.5432477678571429, 0.5398995535714286, 0.5301339285714286, 0.5340401785714286, 0.5265066964285714, 0.5304129464285714, 0.541015625, 0.5443638392857143, 0.537109375, 0.5365513392857143, 0.537109375, 0.5345982142857143, 0.5412946428571429, 0.5396205357142857, 0.541015625, 0.5426897321428571, 0.5415736607142857, 0.54296875, 0.5460379464285714, 0.5460379464285714, 0.5432477678571429, 0.5401785714285714, 0.541015625, 0.5412946428571429, 0.54296875, 0.5435267857142857, 0.5438058035714286, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5407366071428571, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143, 0.5404575892857143]\n",
      "Train acc of epoch:                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5420496323529411, 0.5577895220588235, 0.5630744485294118, 0.5592830882352942, 0.5583639705882353, 0.5611213235294118, 0.5568704044117647, 0.5668658088235294, 0.5697380514705882, 0.5688189338235294, 0.5678998161764706, 0.5678998161764706, 0.5684742647058824, 0.5687040441176471, 0.5666360294117647, 0.5548023897058824, 0.5559512867647058, 0.5448069852941176, 0.5604319852941176, 0.5650275735294118, 0.5647977941176471, 0.5579044117647058, 0.5575597426470589, 0.5560661764705882, 0.5649126838235294, 0.56640625, 0.5623851102941176, 0.5612362132352942, 0.5649126838235294, 0.5652573529411765, 0.5684742647058824, 0.5682444852941176, 0.5674402573529411, 0.5636488970588235, 0.5620404411764706, 0.5619255514705882, 0.5639935661764706, 0.5645680147058824, 0.5647977941176471, 0.5630744485294118, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824, 0.5626148897058824]\n",
      "100%|███████████████████████████████████████████████████████████| 5/5 [13:02<00:00, 157.68s/it, best loss: -0.72265625]\n",
      "Evaluation of best performing model:\n",
      "3584/3584 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "test_score:  1.177380153111049  test_accuracy:  0.7056361607142857\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout': 0.1870009670885795, 'Dropout_1': 2, 'Dropout_2': 0.5054593792632919, 'Dropout_3': 0.6568427252045271, 'Dropout_4': 0.36995205123918895, 'Dropout_5': 0.711408101697044, 'filters': 0, 'filters_1': 2, 'filters_2': 1, 'filters_3': 2, 'filters_4': 2, 'units': 0, 'units_1': 0, 'units_2': 0}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x_train, x_test, y_train, y_test, batch_size, _ ,_, x_train1, x_test1= data()\n",
    "    print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
    "    \n",
    "    best_run, best_model = optim.minimize(model=create_multi_attention_cnn2D_bidirectional_cudnnlstm_multi_input_model_1, data=data ,algo=tpe.suggest, max_evals=5,trials=Trials(), notebook_name='5.51 Multi_Attention_CNN2D_Bidirectional_CuDNNLSTM_multi_input',rseed=1, verbose=False)\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    #best_model.save(\"MULTI_ATTENTION_CNN2D_BIDIRECTIONAL_CUDNNLSTM_MULTI_INPUT_1_bestmodel.h5\")\n",
    "    #print(best_model.get_config())\n",
    "    test_score, test_accuracy = best_model.evaluate([x_test1,x_test], y_test, batch_size=batch_size)\n",
    "    print('test_score: ', test_score, ' test_accuracy: ', test_accuracy)\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GSPC_7days_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
